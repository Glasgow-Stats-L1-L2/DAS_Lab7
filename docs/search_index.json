[["index.html", "Data Analysis: Confidence Intervals and Model Parameter Selection Introduction", " Data Analysis: Confidence Intervals and Model Parameter Selection Introduction In this lab we construct confidence intervals for the parameters in simple and multiple linear regression models. We consider confidence intervals based on theoretical results when standard assumptions hold. We will also consider how to use confidence intervals for variable selection and finish by considering a model selection strategy based on objective measures for model comparisons. Now that you are familiar with RMarkdown, you are encouraged to collate your work in this tutorial in a RMarkdown file. Create a .Rmd file to load the following packages into R: library(dplyr) library(ggplot2) library(janitor) library(moderndive) #library(infer) "],["confidence-intervals-for-regression-parameters.html", "Confidence Intervals for Regression Parameters", " Confidence Intervals for Regression Parameters Let's continue with the teaching evaluations data that we first saw in Lab 4 by fitting the multiple regression with one numerical and one categorical predictor. In this model: \\(y\\): outcome variable of instructor evaluation score predictor variables \\(x_1\\): numerical explanatory/predictor variable of age \\(x_2\\): categorical explanatory/predictor variable of gender evals_multiple &lt;- evals %&gt;% select(score, gender, age) First, recall that we had two competing potential models to explain professors' teaching evaluation scores in Lab 4: Model 1: Parallel lines model (no interaction term) - both male and female professors have the same slope describing the associated effect of age on teaching score Model 2: Interaction model - allowing for male and female professors to have different slopes describing the associated effect of age on teaching score. Recall the plots we made for both these models: Figure 1: Model 1 (No interaction effect included) Figure 2: Model 2 (Interaction effect included) Let's also recall the regression models we fit. First, the regression with no interaction effect: note the use of + in the formula. par.model &lt;- lm(score ~ age + gender, data = evals_multiple) get_regression_table(par.model) Table 1: Model 1 (Regression table with no interaction effect included) term estimate std_error statistic p_value lower_ci upper_ci intercept 4.484 0.125 35.792 0.000 4.238 4.730 age -0.009 0.003 -3.280 0.001 -0.014 -0.003 gender: male 0.191 0.052 3.632 0.000 0.087 0.294 Second, the regression with an interaction effect: note the use of * in the formula. int.model &lt;- lm(score ~ age * gender, data = evals_multiple) get_regression_table(int.model) Table 2: Model 2 (Regression table with interaction effect included) term estimate std_error statistic p_value lower_ci upper_ci intercept 4.883 0.205 23.795 0.000 4.480 5.286 age -0.018 0.004 -3.919 0.000 -0.026 -0.009 gender: male -0.446 0.265 -1.681 0.094 -0.968 0.076 age:gendermale 0.014 0.006 2.446 0.015 0.003 0.024 Notice that, together with the estimated parameter values, the tables include other information about each estimated parameter in the model, namely: std_error: the standard error of each parameter estimate statistic: the test statistic value used to test the null hypothesis that the population parameter is zero p_value: the \\(p\\) value associated with the test statistic under the null hypothesis lower_ci and upper_ci: the lower and upper bounds of the 95% confidence interval for the population parameter These values are calculated using the theoretical results based on the standard assumptions that you will have seen in Regression Modelling in first semester. What is the 95% Confidence Interval for the difference, on average, between the (linear) effect age has on the evaluation scores of male professors and the (linear) effect age has on the evaluation scores of female professors? (0.087, 0.294) (0.003, 0.024) (-0.968, 0.076) (-0.026, -0.009) By just considering the simpler parallel lines model, what can we say about the the difference, on average, between the evaluation scores of male and female professors when age is taken into account? It's highly likely that, on average, male professors' scores are between 0.1 and 0.3 units lower than females professors' scores when age is taken into account It's highly likely that, on average, male professors' scores are between 0.003 and 0.014 units higher than females professors' scores when age is taken into account It's highly likely that, on average, male professors' scores are between 0.003 and 0.014 units lower than females professors' scores when age is taken into account It's highly likely that, on average, male professors' scores are between 0.1 and 0.3 units higher than females professors' scores when age is taken into account "],["inference-using-confidence-intervals.html", "Inference using Confidence Intervals Simple Linear Regression: \\(E(y_i) = \\alpha + \\beta x_i\\) Multiple Regression \\(E(y_i) = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + ...\\)", " Inference using Confidence Intervals We will now interpret confidence intervals of model parameters for the purposes of statistical inference. Simple Linear Regression: \\(E(y_i) = \\alpha + \\beta x_i\\) As we have seen, A confidence interval gives a range of plausible values for a population parameter. We can therefore use the confidence interval for \\(\\beta\\) to state a range of plausible values and, just as usefully, what values are not plausible. The most common values to compare the confidence interval of \\(\\beta\\) with is 0 (zero), since \\(\\beta = 0\\) says there is no (linear) relationship between the outcome variable (\\(y\\)) and the explanatory variable (\\(x\\)). Therefore, if 0 lies within the confidence interval for \\(\\beta\\) then there is insufficient evidence of a linear relationship between \\(y\\) and \\(x\\). However, if 0 does not lie within the confidence interval, then we conclude that \\(\\beta\\) is significantly different from zero and therefore that there is evidence of a linear relationship between \\(y\\) and \\(x\\). Let's use the confidence interval based on theoretical results for slope parameter in the SLR model applied to the teacher evaluation scores with age as the the single explanatory variable and the instructors' evaluation scores as the outcome variable. get_regression_table(slr.model) Table 3: Estimate summaries from the SLR Model of score on age. term estimate std_error statistic p_value lower_ci upper_ci intercept 4.462 0.127 35.195 0.000 4.213 4.711 age -0.006 0.003 -2.311 0.021 -0.011 -0.001 Based on the fitted SLR model, is there evidence that there is a statistically significant linear relationship between the age of the professors and their teaching evaluation score? Yes No Multiple Regression \\(E(y_i) = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + ...\\) Consider, again, the fitted interaction model for score with age and gender as the two explanatory variables. int.model &lt;- lm(score ~ age * gender, data = evals_multiple) get_regression_table(int.model) Table 4: Model 2 (Regression table with interaction effect included) term estimate std_error statistic p_value lower_ci upper_ci intercept 4.883 0.205 23.795 0.000 4.480 5.286 age -0.018 0.004 -3.919 0.000 -0.026 -0.009 gender: male -0.446 0.265 -1.681 0.094 -0.968 0.076 age:gendermale 0.014 0.006 2.446 0.015 0.003 0.024 Based on the fitted interaction model, is there evidence that we should allow for different rates of change for male and female professors' teaching scores as they get older? Yes No "],["variable-selection-using-confidence-intervals.html", "Variable selection using confidence intervals", " Variable selection using confidence intervals When there is more than one explanatory variable in a model, the parameter associated with each explanatory variable is interpreted as the change in the mean response based on a 1-unit change in the corresponding explanatory variable keeping all other variables held constant. Therefore, care must be taken when interpreting the confidence intervals of each parameter by acknowledging that each are plausible values conditional on all the other explanatory variables in the model. Because of the interdependence between the parameter estimates and the variables included in the model, choosing which variables to include in the model is a rather complex task. We will introduce some of the ideas in the simple case where we have 2 potential explanatory variables (\\(x_1\\) and \\(x_2\\)) and use confidence intervals to decide which variables will be useful in predicting the outcome variable (\\(y\\)). One approach is to consider a hierarchy of models: \\[\\hat y_i = \\alpha + \\beta_1 x_{1i} + \\beta_2 x_{2i}\\] \\[\\hat y_i = \\alpha + \\beta_1 x_{1i} \\qquad \\qquad \\qquad \\hat y_i = \\alpha + \\beta_2 x_{2i}\\] \\[\\hat y_i = \\alpha\\] Within this structure we might take a top-down approach: Fit the most general model, i.e. \\(\\hat y_i = \\alpha + \\beta_1 x_{1i} + \\beta_2 x_{2i}\\) since we believe this is likely to provide a good description of the data. Construct confidence intervals for \\(\\beta_1\\) and \\(\\beta_2\\) If both intervals exclude 0 then retain the model with both \\(x_1\\) and \\(x_2\\). If the interval for \\(\\beta_1\\) contains 0 but that for \\(\\beta_2\\) does not, fit the model with \\(x_2\\) alone. If the interval for \\(\\beta_2\\) contains 0 but that for \\(\\beta_1\\) does not, fit the model with \\(x_1\\) alone. If both intervals include 0 it may still be that a model with one variable is useful. In this case the two models with the single variables should be fitted and intervals for \\(\\beta_1\\) and \\(\\beta_2\\) constructed and compared with 0. If we have only a few explanatory variables, then an extension of the strategy outlined above would be effective, i.e. start with the full model and simplify by removing terms until no further terms can be removed. When the number of explanatory variables is large the problem becomes more difficult. We will consider this more challenging situation in the next section. Recall that as well as age and gender, there is also a potential explanatory variable bty_avg in the evals data, i.e. the numerical variable of the average beauty score from a panel of six students' scores between 1 and 10. We can fit the multiple regression model with the two continuous explanatory variables age and bty_avg as follows: mlr.model &lt;- lm(score ~ age + bty_avg, data = evals) Table 5: Esimate summaries from the MLR model with age and bty_avg term estimate std_error statistic p_value lower_ci upper_ci intercept 4.055 0.170 23.870 0.000 3.721 4.389 age -0.003 0.003 -1.148 0.251 -0.008 0.002 bty_avg 0.061 0.017 3.548 0.000 0.027 0.094 Following the process outlined above for choosing which variables to include in the model, what would be your next step after fitting this MLR model? Drop both age and bty_avg from the model Fit a SLR model with age Fit a SLR model with bty_avg Keep the MLR model with both age and bty_avg "],["model-comparisons-using-objective-criteria.html", "Model comparisons using objective criteria", " Model comparisons using objective criteria As was noted in the last section, when the number of potential predictor variables is large the problem of selecting which variables to include in the final model becomes more difficult. The selection of a final regression model always involves a compromise: Predictive accuracy (improved by including more predictors) Parsimony and interpretability (achieved by having less predictors) There are many objective criteria for comparing different models applied to the same data set. All of them trade off the two objectives above, i.e. fit to the data against complexity. Common examples include: The \\(R^2_{adj}\\) values, i.e. the proportions of total variation of response variable explained by the models. \\[R_{adj}^2 = 1 - \\frac{RSS/(n-p-1)}{SST/(n-1)} = 100 \\times \\Bigg[ 1-\\frac{\\sum_{i=1}^n(y_i-\\hat y_i)^2/(n-p-1)}{\\sum_{i=1}^n(y_i-\\bar y_i)^2/(n-1)}\\Bigg]\\] where \\(n\\) is the sample size \\(p\\) is the number of parameters in the model \\(RSS\\) is the residual sum of squares from the fitted model \\(SST\\) is the total sum of squares around the mean response. F ratios and the F-distribution can be used to compare the \\(R_{adj}^2\\) values These can only be used for nested models, i.e. where one model is a particular case of the other Akaike's Information Criteria (AIC) \\[AIC = -2(\\mbox{log-likeihood})+2p = n\\mbox{ln}\\Bigg(\\frac{RSS}{n}\\Bigg)+2p\\] A value based on the maximum likelihood function of the parameters in the fitted model penalized by the number of parameters in the model Can be used to compare any models fitted to the same response variable The smaller the AIC the 'better' the model, i.e. no distributional results are employed to assess differences Bayesian Information Criteria \\[BIC = -2(\\mbox{log-likeihood})+\\mbox{ln}(n)p\\] A popular data analysis strategy which we shall adopt is to calculate \\(R_{adj}^2\\), \\(AIC\\) and \\(BIC\\) and prefer the models which minimize \\(AIC\\) and \\(BIC\\) and that maximize \\(R_{adj}^2\\). To illustrate, let's return to the evals data and the MLR on the teaching evaluation score score with the two continuous explanatory variables age and bty_avg and compare this with the SLR model with just bty_avg. To access these measures for model comparisons we can use the glance() function in the broom package (not to be confused with the glimpse() function in the dplyr package). library(broom) model.comp.values.slr.age &lt;- glance(lm(score ~ age, data = evals)) kable(model.comp.values.slr.age,digits = 2) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.01 0.01 0.54 5.34 0.02 1 -371.81 749.62 762.03 135.09 461 463 model.comp.values.slr.bty_avg &lt;- glance(lm(score ~ bty_avg, data = evals)) kable(model.comp.values.slr.bty_avg,digits = 2) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.04 0.03 0.53 16.73 0 1 -366.22 738.44 750.86 131.87 461 463 model.comp.values.mlr &lt;- glance(lm(score ~ age + bty_avg, data = evals)) kable(model.comp.values.mlr,digits = 2) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.04 0.03 0.53 9.03 0 2 -365.56 739.12 755.67 131.49 460 463 Note that \\(R_{adj}^2\\), \\(AIC\\) and \\(BIC\\) are contained in columns 3, 9 and 10 respectively. To access just these values and combine them in a single table we use: Models &lt;- c(&#39;SLR(age)&#39;,&#39;SLR(bty_avg)&#39;,&#39;MLR&#39;) bind_rows(model.comp.values.slr.age, model.comp.values.slr.bty_avg, model.comp.values.mlr,.id=&quot;Model&quot;) %&gt;% select(Model,adj.r.squared,AIC,BIC) %&gt;% mutate(Model=Models) %&gt;% kable( digits = 2, caption = &quot;Model comparison values for different models&quot; ) Table 6: Model comparison values for different models Model adj.r.squared AIC BIC SLR(age) 0.01 749.62 762.03 SLR(bty_avg) 0.03 738.44 750.86 MLR 0.03 739.12 755.67 Based on these values and the model comparison strategy outlined above, which of these three models would you favour? The MLR model with both age and bty_avg Inconclusive The SLR model with age The SLR model with bty_avg "],["a-final-word-on-model-selection.html", "A final word on model selection", " A final word on model selection A great deal of care should be taken in selecting predictors for a model because the values of the regression coefficients depend upon the variables in the model. Therefore, the predictors included and the order in which they are entered into the model can have a great impact. In an ideal world, predictors should be selected based on past research and new predictors should be added to existing models based on the theoretical importance of the variables. One thing not to do is select hundreds of random predictors, bung them all into a regression analysis and hope for the best. But in practice there are automatic strategies, such as Stepwise and Best Subsets regression, based on systematically searching through the entire list of variables not in the current model to make decisions on whether each should be included. These strategies need to be handled with care, and a proper discussion of them is beyond this course. Our best strategy is a mixture of judgement on what variables should be included as potential explanatory variables, together with parameter interval estimation and a comparison of objective measures for assessing different models. The judgement should be made in the light of advice from the problem context. Golden rule for modelling The key to modelling data is to only use the objective measures as a rough guide. In the end the choice of model will involve your own judgement. You have to be able to defend why you chose a particular model. "],["further-tasks-model-parameter-selection.html", "Further Tasks: Model Parameter Selection", " Further Tasks: Model Parameter Selection Complete the following tasks by using RMarkdown to produce a single document which summarises all your work, i.e. the original questions, your R code, your comments and reflections, etc. Task Data was collected on the characteristics of homes in the American city of Los Angeles (LA) in 2010 and can be found in the file LAhomes.csv on the Moodle page. The data contain the following variables: city - the district of LA where the house was located type - either SFR (Single Family Residences) or Condo/Twh (Condominium/Town House) bed - the number of bedrooms bath - the number of bathrooms garage - the number of car spaces in the garage sqft - the floor area of the house (in square feet) pool - Y if the house has a pool spa - TRUE if the house has a spa price - the most recent sales price ($US) We are interested in exploring the relationships betwen price and the other variables. Read the data into an object called LAhomes and answer the following questions. By looking at the univariate and bivariate distributions on the price and sqft variables below, what would be a sensible way to proceed if we wanted to model this data? What care must be taken if you were to proceed this way? Fit the simple linear model with log(price) as the response and log(sqft) as the predictor. Display the fitted model on a scatterplot of the data and construct a bootstrap confidence interval (using the percentiles of the bootstrap distribution) for the slope parameter in the model and interpret its point and interval estimates. Hint Although you can supply the lm() function with terms like log(price) when you use the infer package to generate bootstrap intervals you the transformed variable needs to already exist. Use the mutate() function in the dplyr package to create new transformed variables. Repeat the analysis in part b. but with the log of the number of bathrooms (bath) as the single explanatory variable. Fit the multiple linear regression model using the log transform of all the variables price (as the response) and both sqft and bath (as the explanatory variables). Calculate the point and interval estimates of the coefficients of the two predictors separately. Compare their point and interval estimates to those you calculated in parts b. and c. Can you account for the differences? Hint Remember that we didn't use bootstrapping to construct the confidence intervals for parameters in multiple linear regression models, but rather used the theoretical results based on assumptions. You can access these estimates using the get_regression_table() function in the moderndive package. Using the objective measures for model comparisons, which of the models in parts b., c. and d. would you favour? Is this consistent with your conclusions in part d.? Task You have been asked to determine the pricing of a New York City (NYC) Italian restaurant's dinner menu such that it is competitively positioned with other high-end Italian restaurants by analyzing pricing data that have been collected in order to produce a regression model to predict the price of dinner. Data from surveys of customers of 168 Italian restaurants in the target area are available. The data can be found in the file restNYC.csv on the Moodle page. Each row represents one customer survey from Italian restaurants in NYC and includes the key variables: Price - price (in $US) of dinner (including a tip and one drink) Food - customer rating of the food (from 1 to 30) Decor - customer rating fo the decor (from 1 to 30) Service - customer rating of the service (from 1 to 30) East - dummy variable with the value 1 if the restaurant is east of Fifth Avenue, 0 otherwise Use the ggpairs function in the GGally package (see the following code) to generate an informative set of graphical and numerical summaries which illuminate the relationships between pairs of variables. Where do you see the strongest evidence of relationships between price and the potential explanatory variables? Is there evidence of multicollineatity in the data? library(GGally) #Package to produce matrix of &#39;pairs&#39; plots and more! restNYC$East &lt;- as.factor(restNYC$East) # East needs to be a factor # Including the `East` factor ggpairs(restNYC[,4:8], aes(colour = East, alpha = 0.4)) # Without the `East` factor ggpairs(restNYC[,4:7], aes(alpha = 0.4)) Fit the simple linear model with Price as the response and Service as the predictor and display the fitted model on a scatterplot of the data. Construct a bootstrap confidence interval (using the standard error from the bootstrap distribution) for the slope parameter in the model. Now fit a multiple regressing model of Price on Service, Food, and Decor. What happens to the significance of Service when additional variables were added to the model? What is the correct interpretation of the coefficient on Service in the linear model which regresses Price on Service, Food, and Decor? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
