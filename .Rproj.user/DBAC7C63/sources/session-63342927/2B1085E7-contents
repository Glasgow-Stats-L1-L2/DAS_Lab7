--- 
title: "Data Analysis: Confidence Intervals and Model Parameter Selection"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
biblio-style: apalike
link-citations: yes
---

# Introduction {-}

Placeholder



<!--chapter:end:index.Rmd-->


# Inference via sampling {-}

Placeholder



<!--chapter:end:01-InferenceViaSampling.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# Inference using sample statistics {-}

The table below lists a variety of scenarios where sample statistics can be used to estimate population parameters. In all 6 scenarios the point estimate/sample statistic *estimates* the unknown population parameter. It does so by computing summary statistics based on a sample of size $n$. We'll cover the first four scenarios in the first half of this week's tutorial and  Scenarios 5 & 6 about the regression parameters in the second half.

```{r inference-summary-table, echo=FALSE, message=FALSE, warning=FALSE}
# Original at https://docs.google.com/spreadsheets/d/1QkOpnBGqOXGyJjwqx1T2O5G5D72wWGfWlPyufOgtkk4/edit#gid=0
library(dplyr)
library(readr)
read_csv("data/ch9_summary_table - Sheet1.csv", na = "") %>% 
  kable(
    caption = "Table 1: Scenarios of sample statisitics for inference", 
    booktabs = TRUE
  )
```

In reality, we don't have access to the population parameter values (if we did, why would we need to estimate them?) we only have a single sample of data from a larger population. We'd like to be able to make some reasonable guesses about population parameters by using a single sample to calculate a range of plausible values for a population parameter. This range of plausible values is known as a **confidence interval** and is the focus of this lab. 

There are [theoretical ways of defining confidence intervals](https://moodle.gla.ac.uk/pluginfile.php/1813455/mod_resource/content/1/Interval%20Estimates%20Summary.pdf) for these different scenarios when certain assumptions hold (such as you saw in [Statistical Inference](https://moodle.gla.ac.uk/course/view.php?id=4742) in Semester 1).  But we can also use a single sample to get some idea of how other samples might vary in terms of their sample statistics, i.e. to estimate the sampling distributions of sample statistics. One common way this is done is via a process known as **bootstrapping** which we turn to now.

<br>
<br>



<!--chapter:end:02-InferenceViaSampleStats.Rmd-->


# Bootstrapping {-}

Placeholder


## Exploratory data analysis {-}
## The Bootstrapping Process {-}

<!--chapter:end:03-Bootstrapping.Rmd-->

```{r include=FALSE, cache=FALSE}
suppressPackageStartupMessages({
  library(webexercises)
})

knitr::knit_hooks$set(webex.hide = function(before, options, envir) {
  if (before) {
    if (is.character(options$webex.hide)) {
      hide(options$webex.hide)
    } else {
      hide()
    }
  } else {
    unhide()
  }
})
```
# The [`infer`](https://www.rdocumentation.org/packages/infer/versions/0.4.0) package for statistical inference {-}

The `infer` package makes great use of the `tidyverse` "pipe" `%>%` to create a pipeline for statistical inference. The goal of the package is to provide a way for its users to explain the computational process of confidence intervals and hypothesis tests using the code as a guide. The verbs build in order here, so you'll want to start with `specify()` and then continue through the others as needed.

## `specify()` {-}

```{r fig.align='center', echo=FALSE}
knitr::include_graphics("images/flowcharts/infer/specify.png")
```

The `specify()` function is used primarily to choose which variables will be the focus of the statistical inference. In addition, a setting of which variable will act as the `explanatory` and which acts as the `response` variable is done here. For proportion problems (i.e. Scenarios 1 & 3 in Table 1) we also specify which of the different levels we are calculating the proportion of (e.g. "females", "approve of Obama's job performance", etc.).

To begin to create a confidence interval for the population mean age of US pennies in 2011, we start by using `specify()` to choose which variable in our `orig_pennies_sample` data we'd like to work with. This can be done in one of two ways:

1. Using the `response` argument:

```{r, eval=FALSE}
orig_pennies_sample %>% 
  specify(response = age_in_2011)
```

2. Using `formula` notation:

```{r, eval=FALSE}
orig_pennies_sample %>% 
  specify(formula = age_in_2011 ~ NULL)
```

Note that the formula notation uses the common R methodology to include the response $y$ variable on the left of the `~` and the explanatory $x$ variable on the right of the "tilde." Recall that you used this notation frequently with the `lm()` function when fitting linear regression models. Either notation works just fine, but a preference is usually given here for the `formula` notation to further build on the ideas from earlier chapters.

## `generate()` {-}

```{r fig.align='center', echo=FALSE}
knitr::include_graphics("images/flowcharts/infer/generate.png")
```

After `specify()`ing the variables we'd like in our inferential analysis, we next feed that into the `generate()` verb. The `generate()` verb's main argument is `reps`, which is used to give how many different repetitions one would like to perform. Another argument here is `type`, which is automatically determined by the kinds of variables passed into `specify()`. We can also be explicit and set this `type` to be `type = "bootstrap"`. Make sure to check out `?generate` to see the options here and use the `?` operator to better understand other verbs as well.

Let's `generate()` 1000 bootstrap samples:

```{r}
thousand_bootstrap_samples <- orig_pennies_sample %>% 
  specify(response = age_in_2011) %>% 
  generate(reps = 1000)
```

We can use the `dplyr` `count()` function to help us understand what the `thousand_bootstrap_samples` data frame looks like:

```{r}
thousand_bootstrap_samples %>% count(replicate)
```

**Note** This is equivalent to `thousand_bootstrap_samples %>% group_by(replicate) %>% summarise(n=n())`

Notice that each `replicate` has 40 entries here. Now that we have 1000 different bootstrap samples, our next step is to `calculate` the bootstrap statistics for each sample.


## `calculate()` {-}

```{r fig.align='center', echo=FALSE}
knitr::include_graphics("images/flowcharts/infer/calculate.png")
```

After `generate()`ing many different samples, we next want to condense those samples down into a single statistic for each `replicate`d sample. As seen in the diagram, the `calculate()` function is helpful here.

As we did at the beginning of this chapter, we now want to calculate the mean `age_in_2011` for each bootstrap sample. To do so, we use the `stat` argument and set it to `"mean"` below. The `stat` argument has a variety of different options here and we will see further examples of this throughout the remaining chapters. 

```{r}
bootstrap_distribution <- orig_pennies_sample %>% 
  specify(response = age_in_2011) %>% 
  generate(reps = 1000) %>% 
  calculate(stat = "mean")
bootstrap_distribution
```

We see that the resulting data has 1000 rows and 2 columns corresponding to the 1000 replicates and the mean for each bootstrap sample.


### Observed statistic {-}

Just as `group_by() %>% summarize()` produces a useful workflow in `dplyr`, we can also use `specify() %>% calculate()` to compute summary measures on our original sample data. It's often helpful both in confidence interval calculations and in hypothesis testing to identify what the corresponding statistic is in the original data. For our example on penny age, we computed above a value of `x_bar` using the `summarize()` verb in `dplyr`:

```{r}
orig_pennies_sample %>% 
  summarize(stat = mean(age_in_2011))
```

This can also be done by skipping the `generate()` step in the pipeline feeding `specify()` directly into `calculate()`:

```{r}
orig_pennies_sample %>% 
  specify(response = age_in_2011) %>% 
  calculate(stat = "mean")
```

This shortcut will be particularly useful when the calculation of the observed statistic is tricky to do using `dplyr` alone. This is particularly the case when working with more than one variable.

## `visualize()` {-}

```{r fig.align='center', echo=FALSE}
knitr::include_graphics("images/flowcharts/infer/visualize.png")
```

The `visualize()` verb provides a simple way to view the bootstrap distribution as a histogram of the `stat` variable values. It has many other arguments that one can use as well including the shading of the histogram values corresponding to the confidence interval values.

```{r}
bootstrap_distribution %>% 
  visualize()
```

The shape of this resulting distribution may look familiar to you.  It resembles the well-known normal (bell-shaped) curve.  It is, in fact, an estimate of the **sampling variability** of the sample statistic.  If you think back to *Statistical Inference* in Semester 1 you will remember that the *Central Limit Theorem* predicted that the sampling distribution would be a **normal distribution**, as seen in the bell-shaped distribution here.

The following diagram recaps the `infer` pipeline for creating a bootstrap distribution.

```{r echo=FALSE, purl=FALSE}
knitr::include_graphics("images/flowcharts/infer/ci_diagram.png")
```

<br>
<br>



<!--chapter:end:04-InferPackage.Rmd-->

