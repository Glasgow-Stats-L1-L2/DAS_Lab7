---
title: "Data Analysis: Confidence Intervals"
output: webexercises::webexercises_default
---

```{r setupCI, include=FALSE}
library(webexercises)
library(ggplot2)
library(dplyr)
library(moderndive)
library(ISLR)
library(skimr)
library(plotly)
library(tidyr)
library(datasets)
library(knitr)
library(janitor)
library(infer)
library(readr)
knitr::opts_chunk$set(comment = NA, warning = FALSE, message = FALSE)
```

```{r setupModelParam, include=FALSE}
library(ggplot2)
library(dplyr)
library(moderndive)
library(ISLR)
library(skimr)
library(plotly)
library(tidyr)
library(datasets)
library(knitr)
library(janitor)
library(infer)
library(readr)
library(broom)
library(gridExtra)
library(GGally) #Package to produce matrix of 'pairs' plots and more!

knitr::opts_chunk$set(comment = NA, warning = FALSE, message = FALSE)
```

# Introduction {-}

In previous weeks we have seen many examples of calculating *sample statistics* such as means, percentiles, standard deviations and regression coefficients.  These *sample statistics* are used as *point estimates* of *population parameters* which describe the *population* from which the *sample* of data was taken.  That last sentence assumes you're familiar with concepts and terminology about sampling (e.g. from the *Statistical Inference* course in 1st Semester) so here is a summary of some key terms:

1. **Population**: The population is a set of $N$ observations of interest.

2. **Population parameter**: A population parameter is a numerical summary value about the population. In most settings, this is a value that's unknown and you wish you knew it.

3. **Census**: An exhaustive enumeration/counting of all observations in the population in order to compute the population parameter's numerical value *exactly*.
    + When $N$ is small, a census is feasible. However, when $N$ is large, a census can get very expensive, either in terms of time, energy, or money. 

4. **Sampling**: Collecting a sample of size $n$ of observations from the population. Typically the sample size $n$ is much smaller than the population size $N$, thereby making sampling a much cheaper procedure than a census. 
    + It is important to remember that the lowercase $n$ corresponds to the sample size and uppercase $N$ corresponds to the population size, thus  $n \leq N$.

5. **Point estimates/sample statistics**: A summary statistic based on the sample of size $n$ that *estimates* the unknown population parameter.

6. **Representative sampling**: A sample is said be a *representative sample* if it "looks like the population". In other words, the sample's characteristics are a good representation of the population's characteristics.

7. **Generalizability**: We say a sample is *generalizable* if any results based on the sample can generalize to the population.

8. **Bias**: In a statistical sense, we say *bias* occurs if certain observations in a population have a higher chance of being sampled than others. We say a sampling procedure is *unbiased* if every observation in a population had an equal chance of being sampled. 

9. **Random sampling**: We say a sampling procedure is *random* if we sample randomly from the population in an unbiased fashion.

***

Now that you are familiar with RMarkdown, you are encouraged to collate your work in this tutorial in a RMarkdown file. Create a `.Rmd` file to load the following packages into R:

```{r message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(janitor)
library(moderndive)
library(infer)
```

**Note**: Additional information and examples can be found in [Chapter 9](https://moderndive.com/9-confidence-intervals.html) of [An Introduction to Statistical and Data Science via R](https://moderndive.com/index.html).

<br>
<br>
 
# Inference via sampling {-}

The logic of inference via sampling is:

* If the sampling of a sample of size $n$ is done at **random**, then
* The sample is **unbiased** and **representative** of the population, thus
* Any result based on the sample can **generalize** to the population, thus
* The **point estimate/sample statistic** is an *estimate* of the unknown population parameter of interest

and thus we have **inferred** something about the population based on our sample.

<br>

**Task** To ground the above concepts, consider the following:

In 2013 National Public Radio in the USA reported a poll of President Obama's approval rating among young Americans aged 18-29 in an article [Poll: Support For Obama Among Young Americans Eroding](https://www.npr.org/sections/itsallpolitics/2013/12/04/248793753/poll-support-for-obama-among-young-americans-eroding). Here is a quote from the article:

> After voting for him in large numbers in 2008 and 2012, young Americans are souring on President Obama.
> 
> According to a new Harvard University Institute of Politics poll, just 41 percent of millennials (adults ages 18-29) approve of Obama's job performance, his lowest-ever standing among the group and an 11-point drop from April.

Identify each of the following terms in this context:

* Population
* Population parameter
* Census
* Sampling
* Point estimates/sample statistics
* Representative sampling
* Generalizability
* Bias
* Random sampling

`r hide("Solution")`
**Population**: Who is the population of $N$ observations of interest?

* Obama poll: $N = \text{?}$ young Americans aged 18-29

**Population parameter**: What is the population parameter? 

* Obama poll: The true population proportion $p$ of young Americans who approve of Obama's job performance.

**Census**: What would a census be in this case? 

* Obama poll: Locating all $N = \text{?}$ young Americans (which is in the millions) and asking them if they approve of Obama's job performance. This would be quite expensive to do!

**Sampling**: How do you acquire the sample of size $n$ observations?

* Obama poll: One way would be to get phone records from a database and pick out $n$ phone numbers. In the case of the above poll, the sample was of size $n=2089$ young adults. 

**Point estimates/sample statistics**: What is the summary statistic based on the sample of size $n$ that *estimates* the unknown population parameter?

* Key: The sample proportion red $\widehat{p}$ of young Americans in the sample of size $n=2089$ that approve of Obama's job performance. In this study's case, $\widehat{p} = 0.41$ which is the quoted 41% figure in the article.

**Representative sampling**: Is the sample procedure *representative*? In other words, to the resulting samples "look like" the population? 

* Obama poll: Does our sample of $n=2089$ young Americans "look like" the population of all young Americans aged 18-29?

**Generalizability**: Are the samples *generalizable* to the greater population?

* Obama poll: Is $\widehat{p} = 0.41$ a "good guess" of $p$? In other words, can we confidently say that 41% of *all* young Americans approve of Obama.

**Bias**: Is the sampling procedure unbiased? In other words, do all observations have an equal chance of being included in the sample?

* Obama poll: Did all young Americans have an equal chance at being represented in this poll? For example, if this was conducted using a database of only mobile phone numbers, would people without mobile phones be included? What about if this were an internet poll on a certain news website? Would non-readers of this this website be included?
    
**Random sampling**: Was the sampling random?

* Obama poll: Random sampling is a necessary assumption for all of the above to work.  Most articles reporting on polls take this assumption as granted. In our Obama poll, you'd have to ask the group that conducted the poll: The Harvard University Institute of Politics.
`r unhide()`

<br>

***

Following "the logic of inference via sampling" (above), in the Obama poll example:

* If we had a way of contacting a randomly chosen sample of 2089 young Americans and poll their approval of Obama, then
* These 2089 young Americans would "look like" the population of all young Americans, thus
* Any results based on this sample of 2089 young Americans can generalize to entire population of all young Americans, thus
* The reported sample approval rating of 41% of these 2089 young Americans is an *estimate* of the true approval rating amongst *all* young Americans.

So this poll's *estimate* of Obama's approval rating amongst millennials was 41%. However is this the end of the story when understanding the results of a poll?  If you read further in the article, it states:

> The poll's margin of error was plus or minus 2.1 percentage points.

Note the term *margin of error*, which here is plus or minus 2.1 percentage points.  This is saying that a typical range of errors for polls of this type is about $\pm 2.1\%$, in words from about 2.1% too small to about 2.1% too big. These errors are caused by *sampling variation*, i.e. the fact that sample statistics vary from sample to sample.

When speaking about estimating population parameters using sample statistics the term "error" can be misleading.  Any variation from the true population parameter value is called "error".  It doesn't mean a mistake has been made, its just acknowledging the fact that an estimate based on a sample is highly likely to be different from the true population parameter it is estimating. A reasonable range of "errors" to expect is called the "margin of error". We'll see in this week's lab that this is what's known as a **95% confidence interval (CI)** for the unknown approval rating (the population parameter in this poll). We'll study confidence intervals (CIs) using a new package for our data science and statistical toolbox: the `infer` package for statistical inference.

<br>
<br>


# Inference using sample statistics {-}

The table below lists a variety of scenarios where sample statistics can be used to estimate population parameters. In all 6 scenarios the point estimate/sample statistic *estimates* the unknown population parameter. It does so by computing summary statistics based on a sample of size $n$. We'll cover the first four scenarios in the first half of this week's tutorial and  Scenarios 5 & 6 about the regression parameters in the second half.

```{r inference-summary-table, echo=FALSE, message=FALSE, warning=FALSE}
# Original at https://docs.google.com/spreadsheets/d/1QkOpnBGqOXGyJjwqx1T2O5G5D72wWGfWlPyufOgtkk4/edit#gid=0
library(dplyr)
library(readr)
read_csv("data/ch9_summary_table - Sheet1.csv", na = "") %>% 
  kable(
    caption = "Table 1: Scenarios of sample statisitics for inference", 
    booktabs = TRUE
  )
```

In reality, we don't have access to the population parameter values (if we did, why would we need to estimate them?) we only have a single sample of data from a larger population. We'd like to be able to make some reasonable guesses about population parameters by using a single sample to calculate a range of plausible values for a population parameter. This range of plausible values is known as a **confidence interval** and is the focus of this lab. 

There are [theoretical ways of defining confidence intervals](https://moodle.gla.ac.uk/pluginfile.php/1813455/mod_resource/content/1/Interval%20Estimates%20Summary.pdf) for these different scenarios when certain assumptions hold (such as you saw in [Statistical Inference](https://moodle.gla.ac.uk/course/view.php?id=4742) in Semester 1).  But we can also use a single sample to get some idea of how other samples might vary in terms of their sample statistics, i.e. to estimate the sampling distributions of sample statistics. One common way this is done is via a process known as **bootstrapping** which we turn to now.

<br>
<br>


# Bootstrapping {-}

The `moderndive` package contains a sample of 40 pennies collected and minted in the United States. Let's explore this sample data first:

```{r include=FALSE}
set.seed(2018)
pennies_sample <- pennies %>% sample_n(40)
```

```{r}
orig_pennies_sample
```

The `orig_pennies_sample` data frame has rows corresponding to a single penny with two variables:

- `year` of minting as shown on the penny and
- `age_in_2011` giving the years the penny had been in circulation in 2011 as an integer, e.g. 15, 2, etc.

Suppose we are interested in understanding some properties of the mean age of **all** US pennies from this data collected in 2011. How might we go about that? Let's begin by understanding some of the properties of `orig_pennies_sample` using data wrangling from Week 2 and data visualization from Week 1.


## Exploratory data analysis {-}

First, let's visualize the values in this sample as a histogram:

```{r}
ggplot(orig_pennies_sample, aes(x = age_in_2011)) +
  geom_histogram(bins = 10, color = "white")
```

We see a roughly symmetric distribution here that has quite a few values near 20 years in age with only a few larger than 40 years or smaller than 5 years. If `orig_pennies_sample` is a representative sample from the population, we'd expect the age of all US pennies collected in 2011 to have a similar shape, a similar spread, and similar measures of central tendency like the mean.

So where does the mean value fall for this sample? This point will be known as our **point estimate** and provides us with a single number that could serve as the guess to what the true population mean age might be. Recall how to find this using the `dplyr` package:

```{r}
x_bar <- orig_pennies_sample %>% 
  summarize(stat = mean(age_in_2011))
x_bar
```

We've denoted this *sample mean* as $\bar{x}$, which is the standard symbol for denoting the mean of a sample. Our point estimate is, thus, $\bar{x} = `r round(x_bar[[1, 1]], 1)`$. Note that this is just one sample providing just one sample mean to estimate the population mean. To construct a **confidence interval** (and to do any sort of *statistical inference* for that matter) we need to know about the **sampling distribution** of this sample mean, i.e. how would its values vary if many samples of the same size were drawn from the same population.

The process of **bootstrapping** allows us to use a single sample to generate many different samples that will act as our way of approximating a sampling distribution using a created **bootstrap distribution** instead. We will "pull ourselves up by our bootstraps" (as the saying goes in English, see [here](https://en.wiktionary.org/wiki/pull_oneself_up_by_one%27s_bootstraps)) using a single sample (`orig_pennies_sample`) to get an idea of the **sampling distribution** of the sample mean.

## The Bootstrapping Process {-}

Bootstrapping uses a process of sampling **with replacement** from our original sample to create new **bootstrap samples** of the *same* size as our original sample. We can use the `rep_sample_n()` function in the `infer` package to explore what one such bootstrap sample would look like. Remember that we are randomly sampling from the original sample here **with replacement** and that we always use the same sample size for the bootstrap samples as the size of the original sample (`orig_pennies_sample`).

```{r include=FALSE}
set.seed(201)
```

```{r}
bootstrap_sample1 <- orig_pennies_sample %>% 
  rep_sample_n(size = 40, replace = TRUE, reps = 1)
bootstrap_sample1
```

Let's visualize what this new bootstrap sample looks like:

```{r}
ggplot(bootstrap_sample1, aes(x = age_in_2011)) +
  geom_histogram(bins = 10, color = "white")
```

We now have another sample from what we could assume comes from the population of interest. We can similarly calculate the sample mean of this bootstrap sample, called a **bootstrap statistic**.

```{r}
bootstrap_sample1 %>% 
  summarize(stat = mean(age_in_2011))
```

<br>

```{r MCQ1, echo=FALSE}
opts_Q1 <- sample(c(answer = "The bootstrap sample mean is smaller than the original sample mean",
                    "The bootstrap sample mean is the same as the original sample mean",
                    "The bootstrap sample mean is larger than the original sample mean"))
```

**Which statement is true?**
`r longmcq(opts_Q1)`

<br>

We'll come back to analyzing the variation in the values of different bootstrap samples' statistics shortly. But first, let's recap what was done to get to this single bootstrap sample using a tactile explanation:

1. First, pretend that each of the 40 values of `age_in_2011` in `orig_pennies_sample` were written on a small piece of paper. Recall that these values were 6, 30, 34, 19, 6, etc.
2. Now, put the 40 small pieces of paper into a receptacle such as a baseball cap.
3. Shake up the pieces of paper.
4. Draw "at random" from the cap to select one piece of paper.
5. Write down the value on this piece of paper. Say that it is 28.
6. Now, place this piece of paper containing 28 back into the cap.
7. Draw "at random" again from the cap to select a piece of paper. Note that this is the *sampling with replacement* part since you may draw 28 again.
8. Repeat this process until you have drawn 40 pieces of paper and written down the values on these 40 pieces of paper. Completing this repetition produces ONE bootstrap sample.

If you look at the values in `bootstrap_sample1`, you can see how this process plays out. We originally drew 28, then we drew 11, then 7, and so on. Of course, we didn't actually use pieces of paper and a cap here. We just had the computer perform this process for us to produce `bootstrap_sample1` using `rep_sample_n()` with `replace = TRUE` set.

The process of *sampling with replacement* is how we can use the original sample to take a guess as to what other values in the population may be. Sometimes in these bootstrap samples, we will select lots of larger values from the original sample, sometimes we will select lots of smaller values, and most frequently we will select values that are near the center of the sample. Let's explore what the distribution of values of `age_in_2011` for six different bootstrap samples looks like to further understand this variability.

```{r}
six_bootstrap_samples <- orig_pennies_sample %>% 
  rep_sample_n(size = 40, replace = TRUE, reps = 6)
```

```{r}
ggplot(six_bootstrap_samples, aes(x = age_in_2011)) +
  geom_histogram(bins = 10, color = "white") +
  facet_wrap(~ replicate)
```

We can also look at the six different means using `dplyr` syntax:

```{r}
six_bootstrap_samples %>% 
  group_by(replicate) %>% 
  summarize(stat = mean(age_in_2011))
```

Instead of doing this six times, we could do it 1000 times and then look at the distribution of `stat` across all 1000 of the `replicate`s. This sets the stage for the `infer` R package (see documentation [here](https://www.rdocumentation.org/packages/infer/versions/0.4.0) or the "Cheat Sheet" on the DA Moodle page) that helps users perform statistical inference such as confidence intervals and hypothesis tests using verbs similar to what you've seen with `dplyr`. In the next section we'll walk through setting up each of the `infer` verbs for confidence intervals using this `orig_pennies_sample` example, while also explaining the purpose of the verbs in a general framework.

<br>
<br>


# The [`infer`](https://www.rdocumentation.org/packages/infer/versions/0.4.0) package for statistical inference {-}

The `infer` package makes great use of the `tidyverse` "pipe" `%>%` to create a pipeline for statistical inference. The goal of the package is to provide a way for its users to explain the computational process of confidence intervals and hypothesis tests using the code as a guide. The verbs build in order here, so you'll want to start with `specify()` and then continue through the others as needed.

## `specify()` {-}

```{r fig.align='center', echo=FALSE}
knitr::include_graphics("images/flowcharts/infer/specify.png")
```

The `specify()` function is used primarily to choose which variables will be the focus of the statistical inference. In addition, a setting of which variable will act as the `explanatory` and which acts as the `response` variable is done here. For proportion problems (i.e. Scenarios 1 & 3 in Table 1) we also specify which of the different levels we are calculating the proportion of (e.g. "females", "approve of Obama's job performance", etc.).

To begin to create a confidence interval for the population mean age of US pennies in 2011, we start by using `specify()` to choose which variable in our `orig_pennies_sample` data we'd like to work with. This can be done in one of two ways:

1. Using the `response` argument:

```{r, eval=FALSE}
orig_pennies_sample %>% 
  specify(response = age_in_2011)
```

2. Using `formula` notation:

```{r, eval=FALSE}
orig_pennies_sample %>% 
  specify(formula = age_in_2011 ~ NULL)
```

Note that the formula notation uses the common R methodology to include the response $y$ variable on the left of the `~` and the explanatory $x$ variable on the right of the "tilde." Recall that you used this notation frequently with the `lm()` function when fitting linear regression models. Either notation works just fine, but a preference is usually given here for the `formula` notation to further build on the ideas from earlier chapters.

## `generate()` {-}

```{r fig.align='center', echo=FALSE}
knitr::include_graphics("images/flowcharts/infer/generate.png")
```

After `specify()`ing the variables we'd like in our inferential analysis, we next feed that into the `generate()` verb. The `generate()` verb's main argument is `reps`, which is used to give how many different repetitions one would like to perform. Another argument here is `type`, which is automatically determined by the kinds of variables passed into `specify()`. We can also be explicit and set this `type` to be `type = "bootstrap"`. Make sure to check out `?generate` to see the options here and use the `?` operator to better understand other verbs as well.

Let's `generate()` 1000 bootstrap samples:

```{r}
thousand_bootstrap_samples <- orig_pennies_sample %>% 
  specify(response = age_in_2011) %>% 
  generate(reps = 1000)
```

We can use the `dplyr` `count()` function to help us understand what the `thousand_bootstrap_samples` data frame looks like:

```{r}
thousand_bootstrap_samples %>% count(replicate)
```

**Note** This is equivalent to `thousand_bootstrap_samples %>% group_by(replicate) %>% summarise(n=n())`

Notice that each `replicate` has 40 entries here. Now that we have 1000 different bootstrap samples, our next step is to `calculate` the bootstrap statistics for each sample.


## `calculate()` {-}

```{r fig.align='center', echo=FALSE}
knitr::include_graphics("images/flowcharts/infer/calculate.png")
```

After `generate()`ing many different samples, we next want to condense those samples down into a single statistic for each `replicate`d sample. As seen in the diagram, the `calculate()` function is helpful here.

As we did at the beginning of this chapter, we now want to calculate the mean `age_in_2011` for each bootstrap sample. To do so, we use the `stat` argument and set it to `"mean"` below. The `stat` argument has a variety of different options here and we will see further examples of this throughout the remaining chapters. 

```{r}
bootstrap_distribution <- orig_pennies_sample %>% 
  specify(response = age_in_2011) %>% 
  generate(reps = 1000) %>% 
  calculate(stat = "mean")
bootstrap_distribution
```

We see that the resulting data has 1000 rows and 2 columns corresponding to the 1000 replicates and the mean for each bootstrap sample.


### Observed statistic {-}

Just as `group_by() %>% summarize()` produces a useful workflow in `dplyr`, we can also use `specify() %>% calculate()` to compute summary measures on our original sample data. It's often helpful both in confidence interval calculations and in hypothesis testing to identify what the corresponding statistic is in the original data. For our example on penny age, we computed above a value of `x_bar` using the `summarize()` verb in `dplyr`:

```{r}
orig_pennies_sample %>% 
  summarize(stat = mean(age_in_2011))
```

This can also be done by skipping the `generate()` step in the pipeline feeding `specify()` directly into `calculate()`:

```{r}
orig_pennies_sample %>% 
  specify(response = age_in_2011) %>% 
  calculate(stat = "mean")
```

This shortcut will be particularly useful when the calculation of the observed statistic is tricky to do using `dplyr` alone. This is particularly the case when working with more than one variable.

## `visualize()` {-}

```{r fig.align='center', echo=FALSE}
knitr::include_graphics("images/flowcharts/infer/visualize.png")
```

The `visualize()` verb provides a simple way to view the bootstrap distribution as a histogram of the `stat` variable values. It has many other arguments that one can use as well including the shading of the histogram values corresponding to the confidence interval values.

```{r}
bootstrap_distribution %>% 
  visualize()
```

The shape of this resulting distribution may look familiar to you.  It resembles the well-known normal (bell-shaped) curve.  It is, in fact, an estimate of the **sampling variability** of the sample statistic.  If you think back to *Statistical Inference* in Semester 1 you will remember that the *Central Limit Theorem* predicted that the sampling distribution would be a **normal distribution**, as seen in the bell-shaped distribution here.

The following diagram recaps the `infer` pipeline for creating a bootstrap distribution.

```{r echo=FALSE, purl=FALSE}
knitr::include_graphics("images/flowcharts/infer/ci_diagram.png")
```

<br>
<br>


# Constructing confidence intervals {-}

>A **confidence interval** gives a range of plausible values for a population parameter.  It depends on a specified _confidence level_ with higher confidence levels corresponding to wider confidence intervals and lower confidence levels corresponding to narrower confidence intervals. Common confidence levels include 90%, 95%, and 99%.

**Confidence intervals** play an important role in the sciences and any field that uses data. You can think of a confidence interval as playing the role of a net when fishing. Instead of just trying to catch a fish with a single spear (estimating an unknown parameter by using a single point estimate/sample statistic), we can use a net to try to provide a range of possible locations for the fish (use a range of possible values based around our sample statistic to make a plausible guess as to the location of the parameter).

The bootstrapping process provides bootstrap statistics that have a bootstrap distribution with center at (or extremely close to) the mean of the original sample. This can be seen by giving the observed statistic `obs_stat` argument the value of the point estimate `x_bar`.

```{r}
bootstrap_distribution %>% 
  visualize(obs_stat = x_bar)
```

We can also compute the mean of the bootstrap distribution of means to see how it compares to `x_bar`:

```{r}
bootstrap_distribution %>% 
  summarize(mean_of_means = mean(stat))
```

<br>

```{r MCQ2, echo=FALSE}
opts_Q2 <- sample(c("The mean of the bootstrap distribution is smaller than the original sample mean",
                    answer = "The mean of the bootstrap distribution is practically the same as the original sample mean",
                    "The mean of the bootstrap distribution is larger than the original sample mean"))
```

**Which statement is true?**
`r longmcq(opts_Q2)`

<br>

As we noted in the previous section, the bootstrap distribution provides an estimate of the sampling distribution of the sample mean, i.e. what the variability in different sample means from different samples of the same size may look like, only using the original sample as our guide. We can quantify this variability in the form of a 95% confidence interval in two different ways.

## 1. The percentile method {-}

One way to calculate a range of plausible values for the unknown mean age of coins in 2011 is to use the middle 95% of the `bootstrap_distribution` to determine our endpoints. Our endpoints are thus at the 2.5^th^ and 97.5^th^ percentiles. This can be done with `infer` using the `get_ci()` function. (You can also use the `conf_int()` or `get_confidence_interval()` functions here as they are aliases that work the exact same way.)

```{r}
bootstrap_distribution %>% 
  get_ci(level = 0.95, type = "percentile")
```

These options are the default values for `level` and `type` so we can also just do:

```{r}
percentile_ci <- bootstrap_distribution %>% 
  get_ci()
percentile_ci
```

Using the percentile method, our range of plausible values for the mean age of US pennies in circulation in 2011 is `r round(percentile_ci[["lower_ci"]],2)` years to `r round(percentile_ci[["upper_ci"]],2)` years. We can use the `shade_confidence_interval()` function to view this using the `endpoints` and `direction` arguments, setting `direction` to `"between"` (between the values) and `endpoints` to be those stored with name `percentile_ci`.

```{r}
bootstrap_distribution %>%
  visualize() +
  shade_confidence_interval(endpoints = percentile_ci, direction = "between")
```

You can see that 95% of the data stored in the `stat` variable in `bootstrap_distribution` falls between the two endpoints with 2.5% to the left outside of the shading and 2.5% to the right outside of the shading. The cut-off points that provide our range are shown with the darker lines.


## 2. The standard error method {-}

If the bootstrap distribution is close to symmetric and bell-shaped, we can also use a shortcut formula for determining the lower and upper endpoints of the confidence interval. This is done by using the formula $\bar{x} \pm (multiplier * SE),$ where $\bar{x}$ is our original sample mean and $SE$ stands for **standard error** and corresponds to the standard deviation of the bootstrap distribution.  

> The *standard error* is the standard deviation of the sampling distribution. 

The variability of the sampling distribution may be approximated by the variability of the bootstrap distribution. Traditional theory-based methodologies for inference also have formulas for standard errors, assuming some conditions are met (you will have seen some of these in [Statistical Inference](https://moodle.gla.ac.uk/pluginfile.php/1813455/mod_resource/content/1/Interval%20Estimates%20Summary.pdf) in Semester 1).

The value of $multiplier$ here is the appropriate percentile of the standard normal distribution. This is automatically calculated when `level` is provided with `level = 0.95` being the default. (95% of the values in a standard normal distribution fall within 1.96 standard deviations of the mean, so `multiplier = 1.96` corresponds to `level = 0.95`, for example.)  As mentioned, this formula assumes that the bootstrap distribution is symmetric and bell-shaped. This is often the case with bootstrap distributions, especially those in which the original distribution of the sample is not highly skewed.

This $\bar{x} \pm (multiplier * SE)$ formula is implemented in the `get_ci()` function as shown with our pennies problem using the bootstrap distribution's variability as an approximation for the sampling distribution's variability. We'll see more on this approximation shortly.

Note that the center of the confidence interval (the `point_estimate`) must be provided for the standard error confidence interval.

```{r eval=FALSE}
standard_error_ci <- bootstrap_distribution %>% 
  get_ci(type = "se", point_estimate = x_bar)
standard_error_ci
```

```{r echo=FALSE}
standard_error_ci <- bootstrap_distribution %>% 
  get_ci(type = "se", point_estimate = x_bar)
round(standard_error_ci, 2)
```

```{r}
bootstrap_distribution %>%
  visualize() +
  shade_confidence_interval(endpoints = standard_error_ci, direction = "between")
```

We see that both methods produce nearly identical confidence intervals with the percentile method being $[`r round(percentile_ci[["lower_ci"]], 2)`, `r round(percentile_ci[["upper_ci"]], 2)`]$ and the standard error method being $[`r round(standard_error_ci[["lower_ci"]], 2)`, `r round(standard_error_ci[["upper_ci"]],2)`]$.

<br>
<br>


# Interpreting the confidence interval {-}

Recall that the confidence intervals we've produced are based on bootstrapping using the single sample `orig_pennies_sample`.  We have been claiming that this is a sample from all the pennies in circulation in 2011, but we can now reveal that it is actually a sample from a larger number of pennies stored as `pennies` in the `moderndive` package. The `pennies` data frame contains 800 rows of data and two columns pertaining to the same variables as `orig_pennies_sample`.  Its important to stress that this is *very artificial*, i.e. we would usually never have access to all the information about the larger group from which our sample is taken, but we have set up the data this way here to illustrate the properties of confidence intervals for the purpose of interpreting confidence intervals.

So let's assume that `pennies` is our population of interest (i.e. a population with $N=800$ units). We can therefore calculate the population mean age of pennies in 2011, denoted by the Greek letter $\mu$, by calculating the mean of `age_in_2011` for the `pennies` data frame.

```{r}
pennies_mu <- pennies %>% 
  summarize(overall_mean = mean(age_in_2011)) %>% 
  pull()  #Use this function to extract the single value from the data frame
pennies_mu
```

As we saw at the end of the previous section, one range of plausible values for the population mean age of pennies in 2011 ($\mu$), is $[`r round(percentile_ci[["lower_ci"]], 2)`, `r round(percentile_ci[["upper_ci"]], 2)`]$. Note that the value $\mu = `r round(pennies_mu,2)`$ (i.e. the mean of `pennies` calculated above) **does** fall in this confidence interval. So in this instance, the confidence interval based on `orig_pennies_sample` was a good estimate of $\mu$.

If we had a different sample of size 40 and constructed a confidence interval using the same method, would we be guaranteed that it contained the population parameter value $\mu$ as well? Let's try it out:

```{r}
pennies_sample2 <- pennies %>% 
  sample_n(size = 40)
```

Note the use of the `sample_n()` function in the `dplyr` package here. This does the same thing as `rep_sample_n(reps = 1)` but omits the extra `replicate` column.

We next create an `infer` pipeline to generate a percentile-based 95% confidence interval for $\mu$:

```{r}
percentile_ci2 <- pennies_sample2 %>% 
  specify(formula = age_in_2011 ~ NULL) %>% 
  generate(reps = 1000) %>% 
  calculate(stat = "mean") %>% 
  get_ci()
percentile_ci2
```

This new confidence interval also contains the value of $\mu$. Let's further investigate by repeating this process 100 times to get 100 different confidence intervals derived from 100 different samples of `pennies`. Each sample will have size of 40 just as the original sample. We will plot each of these confidence intervals as horizontal lines. We will also show a line corresponding to the known population value of `r pennies_mu` years.

```{r echo=FALSE}
set.seed(201)

pennies_samples <- pennies %>% 
  rep_sample_n(size = 40, reps = 100, replace = FALSE)

nested_pennies <- pennies_samples %>% 
  group_by(replicate) %>% 
  tidyr::nest()

infer_pipeline <- function(entry){
  entry %>% 
    specify(formula = age_in_2011 ~ NULL) %>% 
    generate(reps = 1000) %>% 
    calculate(stat = "mean") %>% 
    get_ci()
}

if(!file.exists("rds/pennies_cis.rds")){
  pennies_cis <- nested_pennies %>% 
    mutate(percentile_ci = purrr::map(data, infer_pipeline)) %>% 
    mutate(point_estimate = purrr::map_dbl(data, ~mean(.x$age_in_2011)))
  saveRDS(object = pennies_cis, "rds/pennies_cis.rds")
} else {
  pennies_cis <- readRDS("rds/pennies_cis.rds")
}

perc_cis <- pennies_cis %>% 
  tidyr::unnest(percentile_ci) %>% 
  rename(lower = `2.5%`, upper = `97.5%`) %>% 
  mutate(captured = lower <= pennies_mu & pennies_mu <= upper)

ggplot(perc_cis) +
  geom_point(aes(x = point_estimate, y = replicate, color = captured)) +
  geom_segment(aes(y = replicate, yend = replicate, x = lower, xend = upper, 
                   color = captured)) +
  labs(
    x = expression("Age in 2011 (Years)"),
    y = "Replicate ID",
    title = expression(paste("95% percentile-based confidence intervals for ", 
                             mu, sep = ""))
  ) +
  scale_color_manual(values = c("blue", "orange")) + 
  geom_vline(xintercept = pennies_mu, color = "red") 
```

Of the 100 confidence intervals based on samples of size $n$ = 40, `r sum(perc_cis[["captured"]])` of them captured the population mean $\mu = `r round(pennies_mu,2)`$, whereas `r 100 - sum(perc_cis[["captured"]])` of them did not include it. If we repeated this process of building confidence intervals more times with more samples, we'd expect 95% of them to contain the population mean. In other words, the procedure we have used to generate confidence intervals is "95% reliable" in that we can expect it to include the true population parameter 95% of the time if the process is repeated.

To further accentuate this point, let's perform a similar procedure using 90% confidence intervals instead. This time we will use the standard error method instead of the percentile method for computing the confidence intervals.

```{r echo=FALSE}
set.seed(2019)

pennies_samples2 <- pennies %>% 
  rep_sample_n(size = 40, reps = 100, replace = FALSE)

nested_pennies2 <- pennies_samples2 %>% 
  group_by(replicate) %>% 
  tidyr::nest() %>% 
  mutate(sample_mean = purrr::map_dbl(data, ~mean(.x$age_in_2011)))

bootstrap_pipeline <- function(entry){
  entry %>% 
    specify(formula = age_in_2011 ~ NULL) %>% 
    generate(reps = 1000) %>% 
    calculate(stat = "mean")
}

if(!file.exists("rds/pennies_se_cis.rds")){
  pennies_se_cis <- nested_pennies2 %>%
    mutate(bootstraps = purrr::map(data, bootstrap_pipeline)) %>%
    group_by(replicate) %>%
    mutate(se_ci = purrr::map(bootstraps, get_ci, type = "se",
                              level = 0.9,
                              point_estimate = sample_mean))
  saveRDS(object = pennies_se_cis, "rds/pennies_se_cis.rds")
} else {
  pennies_se_cis <- readRDS("rds/pennies_se_cis.rds")
}

se_cis <- pennies_se_cis %>%
 tidyr::unnest(se_ci) %>%
 mutate(captured = (lower_ci <= pennies_mu & pennies_mu <= upper_ci))

ggplot(se_cis) +
  geom_point(aes(x = sample_mean, y = replicate, color = captured)) +
  geom_segment(aes(y = replicate, yend = replicate, x = lower_ci, xend = upper_ci, 
                   color = captured)) +
  labs(
    x = expression("Age in 2011 (Years)"),
    y = "Replicate ID",
    title = expression(paste(
      "90% standard error-based confidence intervals for ", mu, sep = "")
      )
  ) +
  scale_color_manual(values = c("blue", "orange")) + 
  geom_vline(xintercept = pennies_mu, color = "red") 

```

<br>

```{r MCQ3, echo=FALSE}
opts_Q3 <- sample(c("None of the confidence intervals contained the population mean",
                    answer = "Less confidence intervals than expected contained the population mean",
                    "The number of confidence intervals that contained the population mean was as expected",
                    "More confidence intervals than expected contained the population mean",
                    "All of the confidence intervals contained the population mean"))
```

**Looking at the behaviour of the 100 90% confidence intervals above:**
`r longmcq(opts_Q3)`

<br>

Repeating this process for more samples would result in us getting closer and closer to 90% of the confidence intervals including the true value. It is common to say while interpreting a confidence interval to be "95% confident" or "90% confident" that the specified confidence interval contains the true value. We will use this "confident" language throughout the rest of this chapter, but remember that it is a theoretical statement about what we would expect to happen if we to sample again and again from the same population (which we don't do in practice, of course).

Back to our pennies example; after this elaboration on what the level corresponds to in a confidence interval, let's conclude by providing an interpretation of the original confidence interval result we found in the last section.

> We are 95% confident that the range of values from `r round(percentile_ci[["lower_ci"]],2)` years to `r round(percentile_ci[["upper_ci"]],2)` years contains the true mean age of pennies in circulation in 2011. 

This level of confidence is based on the percentile-based method including the true mean 95% of the time if many different samples (not just the one we used) were collected and confidence intervals were created.

<br>
<br>


# Comparing two proportions {-}

```{r inference-summary-tableRPT, echo=FALSE, message=FALSE, warning=FALSE}
# Original at https://docs.google.com/spreadsheets/d/1QkOpnBGqOXGyJjwqx1T2O5G5D72wWGfWlPyufOgtkk4/edit#gid=0
library(dplyr)
library(readr)
read_csv("data/ch9_summary_table - Sheet1.csv", na = "") %>% 
  kable(
    caption = "Table 1 (repeated): Scenarios of sample statisitics for inference", 
    booktabs = TRUE
  )
```

In the previous sections we have considered Scenario 2 in Table 1 (reproduced here), i.e. constructing a confidence interval for a single population mean.  Often, however, interest lies in comparing two populations, e.g. by constructing a confidence interval for the difference in the population means, as in Scenario 4.  But it may be that the characteristic of interest is a population proportion (rather than a population mean) which is reflected in Scenarios 1 and 3.  In this section we will focus on Scenario 3, i.e. constructing a confidence interval for the difference in two population proportions.

Let's start with an example.  If you see someone else yawn, are you more likely to yawn? In an episode of the TV show [*Mythbusters*](https://en.wikipedia.org/wiki/MythBusters), they tested the myth that yawning is contagious. 

Fifty adults who thought they were being considered for an appearance on the show were interviewed by a show recruiter ("confederate") who either yawned or did not. Participants then sat by themselves in a large van and were asked to wait. While in the van, the Mythbusters watched via hidden camera to see if the unaware participants yawned. The data frame containing the results is available at `mythbusters_yawn` in the `moderndive` package. Let's check it out.

```{r}
mythbusters_yawn
```

- The participant ID is stored in the `subj` variable with values of 1 to 50.
- The `group` variable is either `"seed"` for when a confederate was trying to influence the participant or `"control"` if a confederate did not interact with the participant.
- The `yawn` variable is either `"yes"` if the participant yawned or `"no"` if the participant did not yawn.

We can use the `janitor` package to get a glimpse into this data in a table format:

```{r}
mythbusters_yawn %>% 
  tabyl(group, yawn) %>% 
  adorn_percentages() %>% 
  adorn_pct_formatting() %>% 
  adorn_ns() # To show original counts
```

We are interested in comparing the proportion of those that yawned after seeing a `seed` versus those that yawned with no seed interaction (i.e. `control`). We'd like to see if the difference between these two proportions is significantly larger than 0. If so, we'd have evidence to support the claim that yawning is contagious based on this study.

In looking over this problem, we can take note of some important details to include in our `infer` pipeline:

- We are calling a "`success`" having a `yawn` value of `yes`.
- Our response variable will always correspond to the variable used in the `success` so the response variable is `yawn`.
- The explanatory variable is the other variable of interest here: `group`.


## Compute the point estimate {-}

We are examining the relationship between yawning and whether or not the participant saw someone yawn (`seed`) or not (`control`).

```{r error=TRUE}
mythbusters_yawn %>% 
  specify(formula = yawn ~ group)
```

Note that the `success` argument must be specified in situations such as this where the response variable has only two levels.

```{r}
mythbusters_yawn %>% 
  specify(formula = yawn ~ group, success = "yes")
```

We next want to calculate the statistic of interest for our sample. This corresponds to the difference in the proportion of successes.

```{r error=TRUE, warning=TRUE}
mythbusters_yawn %>% 
  specify(formula = yawn ~ group, success = "yes") %>% 
  calculate(stat = "diff in props")
```

We see another error here. To further check to make sure that R knows exactly what we are after, we need to provide the `order` in which R should subtract these proportions of successes. As the error message states, we'll want to put `"seed"` first after `c()` and then `"control"`: `order = c("seed", "control")`. Our point estimate is thus calculated:

```{r error=TRUE}
obs_diff <- mythbusters_yawn %>% 
  specify(formula = yawn ~ group, success = "yes") %>% 
  calculate(stat = "diff in props", order = c("seed", "control"))
obs_diff
```

This value represents the proportion of those that yawned after seeing a seed yawn (0.2941) minus the proportion of those that yawned with not seeing a seed (0.25).


## Bootstrap distribution {-}

Our next step in building a confidence interval is to create a bootstrap distribution of statistics (differences in proportions of successes). We saw how it works with a single variable in computing bootstrap means in the `pennies` example but we haven't yet worked with bootstrapping involving multiple variables, i.e. comparing two groups. 
In the `infer` package, bootstrapping with multiple variables means that each **row** is potentially resampled. Let's investigate this by looking at the first few rows of `mythbusters_yawn`:

```{r}
head(mythbusters_yawn)
```

When we bootstrap this data, we are potentially pulling the subject's readings multiple times. Thus, we could see the entries of `"seed"` for `group` and `"no"` for `yawn` together in a new row in a bootstrap sample. This is further seen by exploring the `sample_n()` function in `dplyr` on this smaller 6 row data frame comprised of `head(mythbusters_yawn)`. The `sample_n()` function can perform this bootstrapping procedure and is similar to the `rep_sample_n()` function in `infer`, except that it is not `rep`eated but rather only performs one sample with or without replacement.

```{r, echo=FALSE}
set.seed(2019)
```

```{r}
head(mythbusters_yawn) %>% 
  sample_n(size = 6, replace = TRUE)
```

We can see that in this bootstrap sample generated from the first six rows of `mythbusters_yawn`, we have some rows repeated. The same is true when we perform the `generate()` step in `infer` as done below.

```{r, echo=c(1)}
bootstrap_distribution <- mythbusters_yawn %>% 
  specify(formula = yawn ~ group, success = "yes") %>% 
  generate(reps = 1000) %>% 
  calculate(stat = "diff in props", order = c("seed", "control"))

bootstrap_distribution
```

```{r}
bootstrap_distribution %>% 
  visualize(bins = 20)
```

This distribution is roughly symmetric and bell-shaped but isn't quite there. Let's use the percentile-based method to compute a 95% confidence interval for the true difference in the proportion of those that yawn with and without a seed presented. The arguments are explicitly listed here but remember they are the defaults and simply `get_ci()` can be used.

```{r}
bootstrap_distribution %>% 
  get_ci(type = "percentile", level = 0.95)
```

```{r include=FALSE}
bootstrap_distribution %>% 
  get_ci(type = "percentile", level = 0.95) -> myth_ci
```

<br>

```{r MCQ4, echo=FALSE}
opts_Q4 <- sample(c("We have evidence that you are more likely to yawn after seeing someone yawn",
                    answer = "There is insufficient evidence that the probability of  yawning is related to  seeing someone yawn",
                    "We have evidence that you are less likely to yawn after seeing someone yawn"))
```

**Looking at the confidence interval shown here:**
`r longmcq(opts_Q4)`

<br>

Therefore, we are not sure which proportion is larger. Some of the bootstrap statistics showed the proportion without a seed to be higher and others showed the proportion with a seed to be higher. If the confidence interval was entirely above zero, we would be relatively sure (about "95% confident") that the seed group had a higher proportion of yawning than the control group.

Note that this all relates to the importance of denoting the `order` argument in the `calculate()` function. Since we specified `"seed"` and then `"control"` positive values for the statistic correspond to the `"seed"` proportion being higher, whereas negative values correspond to the `"control"` group being higher.

We, therefore, have evidence via this confidence interval suggesting that the conclusion from the Mythbusters show that "yawning is contagious" being "confirmed" is **not** statistically correct.

<br>
<br>


# Further Tasks: Confidence Intervals {-}

You are encouraged to complete the following tasks by using RMarkdown to produce a single document which summarises all your work, i.e. the original questions, your R code, your comments and reflections, etc.

<br>

**Task** In the last section, we constructed a confidence interval for the difference in the proportion of people who yawned between the "seeded" group and the "control" group (Scenario 3).

By modifying the code in the last section in light of how we constructed a confidence interval for the age of pennies in the section on "Constructing confidence intervals" (Scenario 2), use `mythbusters_yawn` data to constuct a confidence interval for the proportion of people who yawn when they see someone else yawn (Scenario 1).  Does this overlap with the confidence interval for the proportion of people who yawn when they did not see someone else yawn (Scenario 1 again!)?  Are your findings here consistent with the findings in the last section?

<br>

**Task** Recall the data on 144 domestic male and female adult cats that we first saw in Week 4.  Each cat had their heart weight in grams (`Hwt`) and body weight in kilograms (`Bwt`) measured, and interest lies in exploring difference between females and males.

a. Construct a bootstrap confidence intervals for the avearge heart weight of female and male cats separately?  Interpret your results.

b. Construct a bootstrap confidence interval for the difference in the avearge heart weights of female and male cats.  Interpret your result. 

c. Repeat a. and b. for the body weight of cats.

`r hide("Hint")`
You need to read in the `cats` data and remind yourself how it is organised, e.g.

```{r data, echo=TRUE, eval=F}
cats <- read.csv("cats.csv")
glimpse(cats)
```
`r unhide()`

<br>
<br>


# Model Parameter Selection {-}

We have begun to consider the construction and use of confidence intervals (CIs) for the population parameters listed in Table 1 (reproduced below).  In particular, we used bootstrap methods to estimate the sampling distributions of the estimates in Scenarios 1-4 and used these to construct CIs for the corresponding population parameters.

```{r inference-summary-table-repeat, echo=FALSE, message=FALSE, warning=FALSE}
# Original at https://docs.google.com/spreadsheets/d/1QkOpnBGqOXGyJjwqx1T2O5G5D72wWGfWlPyufOgtkk4/edit#gid=0
read_csv("data/ch9_summary_table-Sheet1.csv", na = "") %>% 
  kable(
    caption = "Table 1: Scenarios of sample statisitics for inference", 
    booktabs = TRUE
  )
```

Now, we will continue this process for Scenarios 5 and 6, namely construct CIs for the parameters in simple and multiple linear regression models.  We will start with bootstrap methods and also consider CIs based on theoretical results when standard assumptions hold.  We will also consider how to use CIs for variable selection and finish by considering a model selection strategy based on objective measures for model comparisons.

<br>
<br>


# Confidence Intervals for Regression Parameters {-}

## Bootstrap Confidence Intervals for $\beta$ in Simple Linear Regression (SLR) {-}

Just as we did for Scenarios 1-4 in Table 1, we can use the `infer` package to repeatedly sample from a dataset to estimate the sampling distribution and standard error of the estimates of the intercept ($\hat \alpha$) and the covariate's parameter ($\hat \beta$) in the simple linear regression model $\hat y_i = \hat \alpha + \hat \beta x_i$. These sampling distributions enable us to directly find bootstrap confidence intervals for the model parameters.  Usually, interest lies in $\beta$ and so that will be our focus here.

To illustrate this, let's return to the teaching evaluations data that we analyzed last week and start with the SLR model with `age` as the the single explanatory variable and the instructors' evaluation `score`s as the outcome variable.  This data and the fitted model are shown here.

```{r}
slr.model <- lm(score~age, data=evals)
coeff <- slr.model %>% 
  coef() 
coeff
```

```{r modelslr, warning=FALSE, fig.cap="Figure 1: SLR Model applied to Teaching Evaluation Data"}
ggplot(evals, aes(x = age, y = score)) +
  geom_jitter() +
  labs(x = "Age", y = "Teaching Score") +
  geom_smooth(method = "lm", se = FALSE)
```

The point estimate of the slope parameter here is $\hat \beta=$ `r round((slr.model %>% coef() %>% as.numeric)[2],3)`.  The following code estimates the sampling distribution of $\hat \beta$ via the bootstrap method. 

```{r echo=FALSE}
set.seed(201)
```

```{r}
bootstrap_beta_distn <- evals %>% 
  specify(score ~ age) %>%
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "slope")
bootstrap_beta_distn %>% visualize()
```

Now we can use the `get_ci()` function to calculate a 95% confidence interval and a 99% confidence interval. We can do this either using the percentiles of the bootstrap distribution or using an estimate of the standard error from the bootstrap distribution. Remember that both these CIs denote a range of plausible values for the unknown true population slope parameter regressing teaching `score` on `age`.

```{r}
percentile_beta_ci <- bootstrap_beta_distn %>% 
  get_ci(level = 0.95, type = "percentile")
percentile_beta_ci

se_beta_ci <- bootstrap_beta_distn %>% 
  get_ci(level = 0.99, type = "se", point_estimate = coeff[2])
se_beta_ci
```

Using the 2.5% and 97.5% percentiles of the simulated bootstrap sampling distribution the 95% confidence interval is (`r round(as.numeric(percentile_beta_ci)[1],3)`,`r round(as.numeric(percentile_beta_ci[2]),3)`).

<br>

```{r MCQ5, echo=FALSE}
opts_Q5 <- sample(c("(-0.012, -0.001)",
                    answer = "(-0.012, 0.001)",
                    "(-0.011, 0.001)",
                    "(-0.012, -0.011)"))
```

**What is the 99% Confidence Interval for the age parameter by the standard effor approach?**
`r longmcq(opts_Q5)`

<br>

```{r MCQ6, echo=FALSE}
opts_Q6 <- sample(c(answer = "The two confidence intervals are similar since the bootstrap sampling distribution was close to symmetric",
                    "The two confidence intervals are quite different despite the bootstrap sampling distribution being close to symmetric",
                    "The two confidence intervals are similar despite the bootstrap sampling distribution being close to symmetric",
                    "The two confidence intervals are quite different because the bootstrap sampling distribution was close to symmetric"))
```

**Comparing the two different confidence intervals (95% and 99%) produced by the `percentile` and the `se` methods, respectively, we conclude:**
`r longmcq(opts_Q6)`

<br>

## Confidence Intervals for the parameters in multiple regression {-}

Let's continue with the teaching evaluations data by fitting the multiple regression with one numerical and one categorical predictor. In this model:

* $y$: outcome variable of instructor evaluation `score`
* predictor variables
    + $x_1$: numerical explanatory/predictor variable of `age`
    + $x_2$: categorical explanatory/predictor variable of `gender`

```{r}
evals_multiple <- evals %>%
  select(score, gender, age)
```

First, recall that we had two competing potential models to explain professors' teaching evaluation scores in Lab 4:

1. Model 1: Parallel lines model (no interaction term) - both male and female professors have the same slope describing the associated effect of age on teaching score
2. Model 2: Interaction model - allowing for male and female professors to have different slopes describing the associated effect of age on teaching score.

Recall the plots we made for both these models:

```{r model1, echo=FALSE, warning=FALSE, fig.cap="Model 1: No interaction effect included"}
coeff <- lm(score ~ age + gender, data = evals_multiple) %>% coef() %>% as.numeric()
slopes <- evals_multiple %>%
  group_by(gender) %>%
  summarise(min = min(age), max = max(age)) %>%
  mutate(intercept = coeff[1]) %>%
  mutate(intercept = ifelse(gender == "male", intercept + coeff[3], intercept)) %>%
  gather(point, age, -c(gender, intercept)) %>%
  mutate(y_hat = intercept + age * coeff[2])
  
  ggplot(evals_multiple, aes(x = age, y = score, col = gender)) +
  geom_jitter() +
  labs(x = "Age", y = "Teaching Score", color = "Gender") +
  geom_line(data = slopes, aes(y = y_hat), size = 1)
```

```{r model2, echo=FALSE, warning=FALSE, fig.cap="Model 2: Interaction effect included"}
ggplot(evals_multiple, aes(x = age, y = score, col = gender)) +
  geom_jitter() +
  labs(x = "Age", y = "Teaching Score", color = "Gender") +
  geom_smooth(method = "lm", se = FALSE)
```

Let's also recall the regression models we fit. First, the regression with no 
interaction effect: note the use of `+` in the formula.

```{r}
par.model <- lm(score ~ age + gender, data = evals_multiple)

get_regression_table(par.model) %>% 
  knitr::kable(
    digits = 3,
    caption = "Model 1: Regression table with no interaction effect included", 
    booktabs = TRUE
  )
```

Second, the regression with an interaction effect: note the use of `*` in the formula.

```{r}
int.model <- lm(score ~ age * gender, data = evals_multiple)

get_regression_table(int.model) %>% 
  knitr::kable(
    digits = 3,
    caption = "Model 2: Regression table with interaction effect included", 
    booktabs = TRUE
  )
```

Notice that, together with the estimated parameter values, the tables include other information about each estimated parameter in the model, namely:

* **std_error**: the standard error of each parameter estimate
* **statistic**: the test statistic value used to test the null hypothesis that the population parameter is zero
* **p_value**: the $p$ value associated with the test statistic under the null hypothesis
* **lower_ci** and **upper_ci**: the lower and upper bounds of the 95% confidence interval for the population parameter

These values are calculated using the theoretical results based on the standard assumptions that you will have seen in *Regression Modelling* in first semester.  Theses values are **not** based on bootstrapping techniques since these become much harder to implement when working with multiple variables and its beyond the scope of this course.

<br>

```{r MCQ7, echo=FALSE}
opts_Q7 <- sample(c(answer = "(0.003, 0.024)",
                    "(-0.968, 0.076)",
                    "(0.087, 0.294)",
                    "(-0.026,	-0.009)"))
```

**What is the 95% Confidence Interval for the difference, on average, between the (linear) effect age has on the evaluation scores of male professors and the (linear) effect age has on the evaluation scores of female professors?**
`r longmcq(opts_Q7)`

<br>

```{r MCQ8, echo=FALSE}
opts_Q8 <- sample(c(answer = "It's highly likely that, on average, male professors' scores are between 0.1 and 0.3 units higher than females professors' scores when age is taken into account",
                    "It's highly likely that, on average, male professors' scores are between 0.1 and 0.3 units lower than females professors' scores when age is taken into account",
                    "It's highly likely that, on average, male professors' scores are between 0.003 and 0.014	units higher than females professors' scores when age is taken into account",
                    "It's highly likely that, on average, male professors' scores are between 0.003 and 0.014	units lower than females professors' scores when age is taken into account"))
```

**By just considering the simpler parallel lines model, what can we say about the the difference, on average, between the evaluation scores of male and female professors when age is taken into account?**
`r longmcq(opts_Q8)`

<br>
<br>


# Inference using Confidence Intervals {-}

Having described several ways of calculating confidence intervals for model parameters, we are now in a position to interpret them for the purposes of statistical inference.

## Simple Linear Regression {-}  

$\hat y_i = \alpha + \beta x_i$

Whether we have obtained a confidence interval for $\beta$  in a simple linear regression model via bootstrapping or theoretical results based on assumptions, the interpretation of the interval is the same.  As we have seen, 

> A confidence interval gives a range of plausible values for a population parameter.

We can therefore use the confidence interval for $\beta$ to state a range of plausible values and, just as usefully, what values are **not** plausible.  The most common values to compare the confidence interval of $\beta$ with is 0 (zero), since $\beta = 0$ says there is *no* (linear) relationship between the outcome variable ($y$) and the explanatory variable ($x$).  Therefore, if 0 lies within the confidence interval for $\beta$ then there is insufficient evidence of a linear relationship between $y$ and $x$.  However, if 0 does **not** lie within the confidence interval, then we conclude that $\beta$ is significantly different from zero and therefore that there is evidence of a linear relationship between $y$ and $x$.

Let's use the confidence interval based on theoretical results for slope parameter in the SLR model applied to the teacher evaluation scores with `age` as the the single explanatory variable and the instructors' evaluation `score`s as the outcome variable. 

```{r, echo=c(1)}
get_regression_table(slr.model)

get_regression_table(slr.model) %>% 
  knitr::kable(
    digits = 3,
    caption = "Estimate summaries from the SLR Model of `score` on `age`.", 
    booktabs = TRUE
  )
```

<br>

```{r MCQ9, echo=FALSE}
opts_Q9 <- c(answer = "Yes",
             "No")
```

**Based on the fitted SLR model, is there evidence that there is a statistically significant linear relationship between the age of the professors and their teaching evaluation score?**
`r longmcq(opts_Q9)`

<br>

## Multiple Regression {-}

Consider, again, the fitted interaction model for `score` with `age` and `gender` as the two explanatory variables.

```{r, eval=FALSE}
int.model <- lm(score ~ age * gender, data = evals_multiple)
get_regression_table(int.model)
```

```{r, echo=FALSE}
int.model <- lm(score ~ age * gender, data = evals)
get_regression_table(int.model) %>% 
  knitr::kable(
    digits = 3,
    caption = "Model 2: Regression table with interaction effect included", 
    booktabs = TRUE
  )
```

<br>

```{r MCQ10, echo=FALSE}
opts_Q10 <- c(answer = "Yes",
              "No")
```

**Based on the fitted interaction model, is there evidence that we should allow for different rates of change for male and female professors' teaching scores as they get older?**
`r longmcq(opts_Q10)`

<br>
<br>


# Variable selection using confidence intervals {-}

When there is more than one explanatory variable in a model, the parameter associated with each explanatory variable is interpreted as the change in the mean response based on a 1-unit change in the corresponding explanatory variable **keeping all other variables held constant**.  Therefore, care must be taken when interpreting the confidence intervals of each parameter by acknowledging that each are plausible values **conditional on all the other explanatory variables in the model**.

Because of the interdependence between the parameter estimates and the variables included in the model, choosing which variables to include in the model is a rather complex task.  We will introduce some of the ideas in the simple case where we have 2 potential explanatory variables ($x_1$ and $x_2$)  and use confidence intervals to decide which variables will be useful in predicting the outcome variable ($y$).

One approach is to consider a hierarchy of models:

$$\hat y_i = \alpha + \beta_1 x_{1i} + \beta_2 x_{2i}$$   
$$\hat y_i = \alpha + \beta_1 x_{1i} \qquad \qquad \qquad \hat y_i = \alpha + \beta_2 x_{2i}$$   
$$\hat y_i = \alpha$$

Within this structure we might take a top-down approach:

1. Fit the most general model, i.e. $\hat y_i = \alpha + \beta_1 x_{1i} + \beta_2 x_{2i}$ since we believe this is likely to provide a good description of the data.
2. Construct confidence intervals for $\beta_1$ and $\beta_2$
    (a) If both intervals exclude 0 then retain the model with both $x_1$ and $x_2$.
    (b) If the interval for $\beta_1$ contains 0 but that for $\beta_2$ does not, fit the model with $x_2$ alone.
    (c) If the interval for $\beta_2$ contains 0 but that for $\beta_1$ does not, fit the model with $x_1$ alone.
    (d) If both intervals include 0 it may still be that a model with one variable is useful. In this case the two models with the single variables should be fitted and intervals for $\beta_1$ and $\beta_2$ constructed and compared with 0.

If we have only a few explanatory variables, then an extension of the strategy outlined above would be effective, i.e. start with the full model and simplify by removing terms until no further terms can be removed.  When the number of explanatory variables is large the problem becomes more difficult. We will consider this more challenging situation in the next section.

Recall that as well as `age` and `gender`, there is also a potential explanatory variable `bty_avg` in the `evals` data, i.e. the numerical variable of the average beauty score from a panel of six students' scores between 1 and 10. We can fit the multiple regression model with the two continuous explanatory variables `age` and `bty_avg` as follows:

```{r, eval=FALSE}
mlr.model <- lm(score ~ age + bty_avg, data = evals)
```

```{r, echo=FALSE}
mlr.model <- lm(score ~ age + bty_avg, data = evals)
get_regression_table(mlr.model) %>% 
  knitr::kable(
    digits = 3,
    caption = "Esimate summaries from the MLR model with `age` and `bty_avg`", 
    booktabs = TRUE
  )
```

<br>

```{r MCQ11, echo=FALSE}
opts_Q11 <- sample(c(answer = "Fit a SLR model with `bty_avg`",
                     "Fit a SLR model with `age`",
                     "Keep the MLR model with both `age` and `bty_avg`",
                     "Drop both `age` and `bty_avg` from the model"))
```

**Following the process outlined above for choosing which variables to include in the model, what would be your next step after fitting this MLR model?**
`r longmcq(opts_Q11)`

<br>
<br>


# Model comparisons using objective criteria {-}

As was noted in the last section, when the number of potential predictor variables is large the problem of selecting which variables to include in the final model becomes more difficult.  The selection of a final regression model always involves a compromise:

* Predictive accuracy (improved by including more predictors)
* Parsimony and interpretability (achieved by having less predictors)


There are many objective criteria for comparing different models applied to the same data set. All of them trade off the two objectives above, i.e.  fit to the data against complexity. Common examples include:

1.  The $R^2_{adj}$ values, i.e. the proportions of total variation of response variable explained by the models.

$$R_{adj}^2 = 1 - \frac{RSS/(n-p-1)}{SST/(n-1)} = 100 \times \Bigg[ 1-\frac{\sum_{i=1}^n(y_i-\hat y_i)^2/(n-p-1)}{\sum_{i=1}^n(y_i-\bar y_i)^2/(n-1)}\Bigg]$$

  * where 
      - $n$ is the sample size
      - $p$ is the number of parameters in the model
      - $RSS$ is the residual sum of squares from the fitted model
      - $SST$ is the total sum of squares around the mean response.
  * F ratios and the F-distribution can be used to compare the $R_{adj}^2$ values
  * These can only be used for nested models, i.e. where one model is a particular case of the other

2. Akaike's Information Criteria (AIC) 

$$AIC = -2(\mbox{log-likeihood})+2p = n\mbox{ln}\Bigg(\frac{RSS}{n}\Bigg)+2p$$

  * A value based on the maximum likelihood function of the parameters in the fitted model penalized by the number of parameters in the model
  * Can be used to compare any models fitted to the same response variable 
  * The smaller the AIC the 'better' the model, i.e. no distributional results are employed to assess differences 

3. Bayesian Information Criteria 

$$BIC = -2(\mbox{log-likeihood})+\mbox{ln}(n)p$$
A popular data analysis strategy which we shall adopt is to calculate $R_{adj}^2$, $AIC$ and $BIC$ and prefer the models which **minimize** $AIC$ and $BIC$ and that **maximize** $R_{adj}^2$.

To illustrate, let's return to the `evals` data and the MLR on the teaching evaluation score `score` with the two continuous explanatory variables `age` and `bty_avg` and compare this with the SLR model with just `bty_avg`.  To access these measures for model comparisons we can use the `glance()` function in the `broom` package (not to be confused with the `glimpse()` function in the `dplyr` package).

```{r}
library(broom)
model.comp.values.slr.age <- glance(lm(score ~ age, data = evals))
model.comp.values.slr.age
model.comp.values.slr.bty_avg <- glance(lm(score ~ bty_avg, data = evals))
model.comp.values.slr.bty_avg
model.comp.values.mlr <- glance(lm(score ~ age + bty_avg, data = evals))
model.comp.values.mlr
```

Note that $R_{adj}^2$, $AIC$ and $BIC$ are contained in columns 3, 9 and 10 respectively.  To access just these values and combine them in a single table we use:

```{r}
Models <- c('SLR(age)','SLR(bty_avg)','MLR') 
bind_rows(model.comp.values.slr.age, model.comp.values.slr.bty_avg, 
          model.comp.values.mlr,.id="Model") %>%
  select(Model,adj.r.squared,AIC,BIC) %>%
  mutate(Model=Models) %>%  
  kable(
     digits = 2,
     caption = "Model comparison values for different models" 
  )
```

<br>

```{r MCQ12, echo=FALSE}
opts_Q12 <- sample(c("The SLR model with `age`",
                     answer = "The SLR model with `bty_avg`",
                     "The MLR model with both `age` and `bty_avg`",
                     "Inconclusive"))
```

**Based on these values and the model comparison strategy outlined above, which of these three models would you favour?**
`r longmcq(opts_Q12)`

<br>
<br>


# A final word on model selection {-}

A great deal of care should be taken in selecting predictors for a model because the values of the regression coefficients depend upon the variables in the model. Therefore, the predictors included and the order in which they are entered into the model can have a great impact. In an ideal world, predictors should be selected based on past research and new predictors should be added to existing models based on the theoretical importance of the variables.  One thing not to do is select hundreds of random predictors, bung them all into a regression analysis and hope for the best. 

But in practice there are automatic strategies, such as *Stepwise* and *Best Subsets* regression, based on systematically searching through the entire list of variables not in the current model to make decisions on whether each should be included. These strategies need to be handled with care, and a proper discussion of them is beyond this course. Our best strategy is a mixture of judgement on what variables should be included as potential explanatory variables, together with parameter interval estimation and a comparison of objective measures for assessing different models. The judgement should be made in the light of advice from the problem context.

**Golden rule for modelling**

> The key to modelling data is to only use the objective measures as a rough guide. In the end the choice of model will involve your own judgement. You have to be able to defend why you chose a particular model.

<br>
<br>


# Further Tasks: Model Parameter Selection {-}

Complete the following tasks by using RMarkdown to produce a single document which summarises all your work, i.e. the original questions, your R code, your comments and reflections, etc.

<br>

**Task** Data was collected on the characteristics of homes in the American city of Los Angeles (LA) in 2010 and can be found in the file `LAhomes.csv` on the Moodle page.  The data contain the following variables:

* `city` - the district of LA where the house was located
* `type` - either `SFR` (Single Family Residences) or `Condo/Twh` (Condominium/Town House)
* `bed` - the number of bedrooms
* `bath` - the number of bathrooms
* `garage` - the number of car spaces in the garage
* `sqft` - the floor area of the house (in square feet)
* `pool` - `Y` if the house has a pool
* `spa` - `TRUE` if the house has a spa
* `price` - the most recent sales price ($US)

We are interested in exploring the relationships betwen `price` and the other variables.

Read the data into an object called `LAhomes` and answer the following questions.

a. By looking at the univariate and bivariate distributions on the `price` and `sqft` variables below, what would be a sensible way to proceed if we wanted to model this data?  What care must be taken if you were to proceed this way?

```{r, echo=F, warning=FALSE, message=FALSE}
LAhomes <- read_csv("LAhomes.csv")
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(gridExtra)

hist1 <- ggplot(LAhomes,aes(x=price))+
  geom_histogram()

hist2 <- ggplot(LAhomes,aes(x=sqft))+
  geom_histogram()

hist1log <- ggplot(LAhomes,aes(x=log(price)))+
  geom_histogram()

hist2log <- ggplot(LAhomes,aes(x=log(sqft)))+
  geom_histogram()

plot1 <- ggplot(LAhomes,aes(x=sqft,y=price))+
  geom_point()

plot2 <- ggplot(LAhomes,aes(x=log(sqft),y=log(price)))+
  geom_point()

grid.arrange(hist1, hist2, hist1log, hist2log, plot1, plot2, ncol=2, nrow=3)
```

b. Fit the simple linear model with `log(price)` as the response and `log(sqft)` as the predictor. Display the fitted model on a scatterplot of the data and construct a bootstrap confidence interval (using the percentiles of the bootstrap distribution) for the slope parameter in the model and interpret its point and interval estimates.

`r hide("Hint")`
Although you can supply the `lm()` function with terms like `log(price)` when you use the `infer` package to generate bootstrap intervals you the transformed variable needs to already exist.  Use the `mutate()` function in the `dplyr` package to create new transformed variables.
`r unhide()`

c. Repeat the analysis in part b. but with the log of the number of bathrooms (`bath`) as the single explanatory variable.

d. Fit the multiple linear regression model using the **log transform of all the variables** `price` (as the response) and both `sqft` and `bath` (as the explanatory variables). Calculate the point and interval estimates of the coefficients of the two predictors separately. Compare their point and interval estimates to those you calculated in parts b. and c.   Can you account for the differences?

`r hide("Hint")`
Remember that we didn't use bootstrapping to construct the confidence intervals for parameters in multiple linear regression models, but rather used the theoretical results based on assumptions.  You can access these estimates using the `get_regression_table()` function in the `moderndive` package.
`r unhide()`

e. Using the objective measures for model comparisons, which of the models in parts b., c. and d. would you favour?  Is this consistent with your conclusions in part d.?

<br>

**Task** You have been asked to determine the pricing of a New York City (NYC) Italian restaurant's dinner menu such that it is competitively positioned with other high-end Italian restaurants by analyzing pricing data that have been collected in order to produce a regression model to predict the price of dinner.

Data from surveys of customers of 168 Italian restaurants in the target area are available. The data can be found in the file `restNYC.csv` on the Moodle page.  Each row represents one customer survey from Italian restaurants in NYC and includes the key variables:

* `Price` - price (in $US) of dinner (including a tip and one drink)
* `Food` - customer rating of the food (from 1 to 30)
* `Decor` - customer rating fo the decor (from 1 to 30)
* `Service` - customer rating of the service (from 1 to 30)
* `East` - dummy variable with the value 1 if the restaurant is east of Fifth Avenue, 0 otherwise

```{r, echo=F, warning=FALSE, message=FALSE}
restNYC <- read_csv("restNYC.csv")
```

a. Use the `ggpairs` function in the `GGally` package (see the following code) to generate an informative set of graphical and numerical summaries which illuminate the relationships between pairs of variables.  Where do you see the strongest evidence of relationships between `price` and the potential explanatory variables?  Is there evidence of multicollineatity in the data?

```{r, eval=FALSE}
library(GGally) #Package to produce matrix of 'pairs' plots and more!
restNYC$East <- as.factor(restNYC$East) # East needs to be a factor
# Including the `East` factor
ggpairs(restNYC[,4:8], aes(colour = East, alpha = 0.4)) 
# Without the `East` factor
ggpairs(restNYC[,4:7], aes(alpha = 0.4)) 
```

b. Fit the simple linear model with `Price` as the response and `Service` as the predictor and  display the fitted model on a scatterplot of the data.   Construct a bootstrap confidence interval (using the standard error from the bootstrap distribution) for the slope parameter in the model.

    Now fit a multiple regressing model of `Price` on `Service`, `Food`, and `Decor`.  What happens to the significance of `Service` when additional variables were added to the model?
    
c. What is the correct interpretation of the coefficient on `Service` in the linear model which regresses `Price` on `Service`, `Food`, and `Decor`?

<br>
<br>



