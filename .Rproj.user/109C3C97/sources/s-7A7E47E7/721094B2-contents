# Confidence Intervals for Regression Parameters {-}

<!-- ## Bootstrap Confidence Intervals for $\beta$ in Simple Linear Regression (SLR) {-} -->

<!-- Just as we did for Scenarios 1-4 in Table 1, we can use the `infer` package to repeatedly sample from a dataset to estimate the sampling distribution and standard error of the estimates of the intercept ($\hat \alpha$) and the covariate's parameter ($\hat \beta$) in the simple linear regression model $\hat y_i = \hat \alpha + \hat \beta x_i$. These sampling distributions enable us to directly find bootstrap confidence intervals for the model parameters.  Usually, interest lies in $\beta$ and so that will be our focus here. -->

<!-- To illustrate this, let's return to the teaching evaluations data that we analyzed last week and start with the SLR model with `age` as the the single explanatory variable and the instructors' evaluation `score`s as the outcome variable.  This data and the fitted model are shown here. -->

```{r, echo=FALSE}
slr.model <- lm(score~age, data=evals)
coeff <- slr.model %>%
  coef()
#coeff
```

<!-- BOOTSTRAP MATERIAL REMOVED -->

<!-- ## Confidence Intervals for the parameters in multiple regression {-} -->

Let's continue with the teaching evaluations data that we first saw in Lab 4 by fitting the multiple regression with one numerical and one categorical predictor. In this model:

* $y$: outcome variable of instructor evaluation `score`
* predictor variables
    + $x_1$: numerical explanatory/predictor variable of `age`
    + $x_2$: categorical explanatory/predictor variable of `gender`

```{r}
evals_multiple <- evals %>%
  select(score, gender, age)
```

First, recall that we had two competing potential models to explain professors' teaching evaluation scores in Lab 4:

1. Model 1: Parallel lines model (no interaction term) - both male and female professors have the same slope describing the associated effect of age on teaching score
2. Model 2: Interaction model - allowing for male and female professors to have different slopes describing the associated effect of age on teaching score.

Recall the plots we made for both these models:

```{r model1, echo=FALSE, warning=FALSE, fig.cap="Model 1 (No interaction effect included)"}
coeff <- lm(score ~ age + gender, data = evals_multiple) %>% coef() %>% as.numeric()
slopes <- evals_multiple %>%
  group_by(gender) %>%
  summarise(min = min(age), max = max(age)) %>%
  mutate(intercept = coeff[1]) %>%
  mutate(intercept = ifelse(gender == "male", intercept + coeff[3], intercept)) %>%
  gather(point, age, -c(gender, intercept)) %>%
  mutate(y_hat = intercept + age * coeff[2])
  
  ggplot(evals_multiple, aes(x = age, y = score, col = gender)) +
  geom_jitter() +
  labs(x = "Age", y = "Teaching Score", color = "Gender") +
  geom_line(data = slopes, aes(y = y_hat), size = 1)
```

```{r model2, echo=FALSE, warning=FALSE, fig.cap="Model 2 (Interaction effect included)"}
ggplot(evals_multiple, aes(x = age, y = score, col = gender)) +
  geom_jitter() +
  labs(x = "Age", y = "Teaching Score", color = "Gender") +
  geom_smooth(method = "lm", se = FALSE)
```

Let's also recall the regression models we fit. First, the regression with no 
interaction effect: note the use of `+` in the formula.

```{r, echo=TRUE, eval=FALSE}
par.model <- lm(score ~ age + gender, data = evals_multiple)

get_regression_table(par.model)
```

```{r, echo=FALSE}
par.model <- lm(score ~ age + gender, data = evals_multiple)

get_regression_table(par.model) %>% 
  knitr::kable(
    digits = 3,
    caption = "Model 1 (Regression table with no interaction effect included)", 
    booktabs = TRUE
  )
```

Second, the regression with an interaction effect: note the use of `*` in the formula.

```{r, echo=TRUE, eval=FALSE}
int.model <- lm(score ~ age * gender, data = evals_multiple)

get_regression_table(int.model)
```

```{r, echo=FALSE}
int.model <- lm(score ~ age * gender, data = evals_multiple)

get_regression_table(int.model) %>% 
  knitr::kable(
    digits = 3,
    caption = "Model 2 (Regression table with interaction effect included)", 
    booktabs = TRUE
  )
```

Notice that, together with the estimated parameter values, the tables include other information about each estimated parameter in the model, namely:

* **std_error**: the standard error of each parameter estimate
* **statistic**: the test statistic value used to test the null hypothesis that the population parameter is zero
* **p_value**: the $p$ value associated with the test statistic under the null hypothesis
* **lower_ci** and **upper_ci**: the lower and upper bounds of the 95% confidence interval for the population parameter

These values are calculated using the theoretical results based on the standard assumptions that you will have seen in *Regression Modelling* in first semester.  
<!-- Theses values are **not** based on bootstrapping techniques since these become much harder to implement when working with multiple variables and its beyond the scope of this course. -->

<br>

```{r MCQ7, echo=FALSE}
opts_Q7 <- sample(c(answer = "(0.003, 0.024)",
                    "(-0.968, 0.076)",
                    "(0.087, 0.294)",
                    "(-0.026,	-0.009)"))
```

**What is the 95% Confidence Interval for the difference, on average, between the (linear) effect age has on the evaluation scores of male professors and the (linear) effect age has on the evaluation scores of female professors?**
`r longmcq(opts_Q7)`

<br>

```{r MCQ8, echo=FALSE}
opts_Q8 <- sample(c(answer = "It's highly likely that, on average, male professors' scores are between 0.1 and 0.3 units higher than females professors' scores when age is taken into account",
                    "It's highly likely that, on average, male professors' scores are between 0.1 and 0.3 units lower than females professors' scores when age is taken into account",
                    "It's highly likely that, on average, male professors' scores are between 0.003 and 0.014	units higher than females professors' scores when age is taken into account",
                    "It's highly likely that, on average, male professors' scores are between 0.003 and 0.014	units lower than females professors' scores when age is taken into account"))
```

**By just considering the simpler parallel lines model, what can we say about the the difference, on average, between the evaluation scores of male and female professors when age is taken into account?**
`r longmcq(opts_Q8)`

<br>
<br>


